{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "实例化后, 不用调用实例方法, 直接实例(), 将实例作为函数.\n",
    "    这是因为在nn.Module中我们实现了__call__等价于那个forward函数.  \n",
    "    net() <==> net.forward() 同理.\n",
    "\n",
    "自定义的激活函数如果是非可导的话, auto-grad是否可以求出导数, 还是必须自定义导数. \n",
    "    那么其实, 不可导的函数是非常少的. 我们经常说的都是在某个点不可导, 不过在我们计算机中, 我们都是数值解.\n",
    "    都是逼近, 模拟, 所以不可导的话, 我们随便给个近似值就可以了. \n",
    "\n",
    " 一般使用GPU训练, data在哪一步to gpu比较好:\n",
    "    一般是在最后, 也就是在net.to()之前. 我们需要先将data to gpu  \n",
    "\n",
    "\n",
    "CUDA和GPU的关系:\n",
    "    cuda是sdk, 也就是编译的那个东西. GPU当然是一个硬件了. \n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
