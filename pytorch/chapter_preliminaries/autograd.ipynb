{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "724521e3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 自动微分\n",
    "# 自动求导\n",
    ":label:`sec_autograd`\n",
    "\n",
    "正如我们在 :numref:`sec_calculus`中所说的那样，求导是几乎所有深度学习优化算法的关键步骤。\n",
    "虽然求导的计算很简单，只需要一些基本的微积分。\n",
    "但对于复杂的模型，手工进行更新是一件很痛苦的事情（而且经常容易出错）。\n",
    "\n",
    "深度学习框架通过自动计算导数，即*自动微分*（automatic differentiation）来加快求导。\n",
    "实际中，根据我们设计的模型，系统会构建一个*计算图*（computational graph），\n",
    "来跟踪计算是哪些数据通过哪些操作组合起来产生输出。\n",
    "自动微分使系统能够随后反向传播梯度。\n",
    "这里，*反向传播*（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。\n",
    "\n",
    "## 一个简单的例子\n",
    "\n",
    "作为一个演示例子，(**假设我们想对函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于列向量$\\mathbf{x}$求导**)。\n",
    "首先，我们创建变量`x`并为其分配一个初始值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b5f24a5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch # import torch. 导入torch\n",
    "\n",
    "x = torch.arange(4.0) # 生成一个序列.  \n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47bf625",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[**在我们计算$y$关于$\\mathbf{x}$的梯度之前，我们需要一个地方来存储梯度。**]\n",
    "重要的是，我们不会在每次对一个参数求导时都分配新的内存。\n",
    "因为我们经常会成千上万次地更新相同的参数，每次都分配新的内存可能很快就会将内存耗尽。\n",
    "注意，一个标量函数关于向量$\\mathbf{x}$的梯度是向量，并且与$\\mathbf{x}$具有相同的形状。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "043bca7d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# 在我们计算 𝑦 关于 𝐱 的梯度之前，我们需要一个地方来存储梯度。\n",
    "# 因此, 我们需要指定: requires_grad()来告诉torch, 这个变量我需要保存梯度. 然后我们就可以通过.grad查看梯度了。\n",
    "\n",
    "x.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True)\n",
    "x.grad  # 默认值是None \n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee2d6c8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "(**现在让我们计算$y$。**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a604995",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 2 * torch.dot(x, x) # 就是2*x^2\n",
    "# 求导后: 4*x. \n",
    "y\n",
    "# 因为是隐式的求解梯度, 因此这里是有一个函数: grad_fn来求解梯度. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a93c959",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "`x`是一个长度为4的向量，计算`x`和`x`的点积，得到了我们赋值给`y`的标量输出。\n",
    "接下来，我们[**通过调用反向传播函数来自动计算`y`关于`x`每个分量的梯度**]，并打印这些梯度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "141be2b1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward() # 调用反向传播来查看y关于x的每一个分量的梯度. \n",
    "x.grad # 通过x.grad访问一下在x每个分量的导数."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2421e84d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于$\\mathbf{x}$的梯度应为$4\\mathbf{x}$。\n",
    "让我们快速验证这个梯度是否计算正确。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8edb0bac",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b417d9ed",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[**现在让我们计算`x`的另一个函数。**]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7827f820",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在默认情况下，PyTorch会累积梯度，所以我们需要清除之前的值\n",
    "x.grad.zero_() # 在pytorch中_表示原地操作, 表示对当前变量进行原地操作. \n",
    "# 对梯度清零.\n",
    "print(x.grad) #\n",
    "# 重新计算: y的函数: \n",
    "y = x.sum() # 标量.\n",
    "y.backward() # backward calculate\n",
    "x.grad # get gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13649b57",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 非标量变量的反向传播\n",
    "\n",
    "当`y`不是标量时，向量`y`关于向量`x`的导数的最自然解释是一个矩阵。\n",
    "对于高阶和高维的`y`和`x`，求导的结果可以是一个高阶张量。\n",
    "\n",
    "然而，虽然这些更奇特的对象确实出现在高级机器学习中（包括[**深度学习中**]），\n",
    "但当我们调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。\n",
    "这里(**，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和。**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b49bf8c7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>), tensor([0., 2., 4., 6.]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。\n",
    "# 在我们的例子中，我们只想求偏导数的和，所以传递一个1的梯度是合适的\n",
    "x.grad.zero_() # 清空. \n",
    "y = x * x # 这是一个向量. 默认是元素运算. 这不是一个标量.\n",
    "# 那么理论上, 他的梯度是一个矩阵. 但是在深度学习中我们很少做这个事情.\n",
    "# 等价于y.backward(torch.ones(len(x)))\n",
    "y.sum().backward() # 我们在绝大多数情况都是对一个标量进行求导, 我们不会对一个vector或者一个matrix求导. \n",
    "# 因此我们会先使用sum()将任何维度转换成一个标量. 然后在求导. \n",
    "y, x.grad\n",
    "# 我们后面会讲在什么情况下进行求和."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe48ad69",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 分离计算\n",
    "\n",
    "有时，我们希望[**将某些计算移动到记录的计算图之外**]。\n",
    "例如，假设`y`是作为`x`的函数计算的，而`z`则是作为`y`和`x`的函数计算的。\n",
    "想象一下，我们想计算`z`关于`x`的梯度，但由于某种原因，我们希望将`y`视为一个常数，\n",
    "并且只考虑到`x`在`y`被计算后发挥的作用。\n",
    "\n",
    "在这里，我们可以分离`y`来返回一个新变量`u`，该变量与`y`具有相同的值，\n",
    "但丢弃计算图中如何计算`y`的任何信息。\n",
    "换句话说，梯度不会向后流经`u`到`x`。\n",
    "因此，下面的反向传播函数计算`z=u*x`关于`x`的偏导数，同时将`u`作为常数处理，\n",
    "而不是`z=x*x*x`关于`x`的偏导数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc6d556e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我可以将一些计算移动到计算图之外. \n",
    "x.grad.zero_() # empty gradient\n",
    "y = x * x # definite y function\n",
    "u = y.detach() # detach抽取出来. 就是y看作一个scalar. 是一个常数, \n",
    "# y.detach()此时就不是一个关于x的函数, 而仅仅是x*x的一个数\n",
    "z = u * x # z就是一个常数*u\n",
    "# 那么求导之后, 关于x的导数就是u\n",
    "\n",
    "z.sum().backward()\n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b097232",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "关于.detach(), 就是cut branch. 剪枝操作, 我们不需要一些网络中的参数时候, 或者我们需要将一些参数固定住的时候, 那么detach是非常有用的. \n",
    "detach: 拆下, 使分离, 脱离, 摆脱. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af7fb8d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "由于记录了`y`的计算结果，我们可以随后在`y`上调用反向传播，\n",
    "得到`y=x*x`关于的`x`的导数，即`2*x`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8c8c798",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "# 此时y还是x的函数, 因此对y进行backward后, 得到的还是关于x的导数. \n",
    "y.sum().backward()\n",
    "x.grad == 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a826c57",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Python控制流的梯度计算\n",
    "\n",
    "使用自动微分的一个好处是：\n",
    "[**即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度**]。\n",
    "在下面的代码中，`while`循环的迭代次数和`if`语句的结果都取决于输入`a`的值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44a4eaf2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000: # norm()就是范数. \n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f3a4e8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T02:37:06.673776Z",
     "iopub.status.busy": "2022-07-31T02:37:06.673308Z",
     "iopub.status.idle": "2022-07-31T02:37:06.677303Z",
     "shell.execute_reply": "2022-07-31T02:37:06.676673Z"
    },
    "origin_pos": 38,
    "pycharm": {
     "name": "#%%\n"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000: # norm()就是范数. \n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae128b06",
   "metadata": {
    "origin_pos": 40,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "让我们计算梯度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e25d3408",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T02:37:06.680229Z",
     "iopub.status.busy": "2022-07-31T02:37:06.679765Z",
     "iopub.status.idle": "2022-07-31T02:37:06.684566Z",
     "shell.execute_reply": "2022-07-31T02:37:06.683900Z"
    },
    "origin_pos": 42,
    "pycharm": {
     "name": "#%%\n"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "a = torch.randn(size=(), requires_grad=True)\n",
    "d = f(a)\n",
    "d.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eb899a",
   "metadata": {
    "origin_pos": 44,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "我们现在可以分析上面定义的`f`函数。\n",
    "请注意，它在其输入`a`中是分段线性的。\n",
    "换言之，对于任何`a`，存在某个常量标量`k`，使得`f(a)=k*a`，其中`k`的值取决于输入`a`。\n",
    "因此，我们可以用`d/a`验证梯度是否正确。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7df0be3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T02:37:06.687918Z",
     "iopub.status.busy": "2022-07-31T02:37:06.687447Z",
     "iopub.status.idle": "2022-07-31T02:37:06.692256Z",
     "shell.execute_reply": "2022-07-31T02:37:06.691639Z"
    },
    "origin_pos": 46,
    "pycharm": {
     "name": "#%%\n"
    },
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad == d / a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656f59b1",
   "metadata": {
    "origin_pos": 48,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 小结\n",
    "\n",
    "* 深度学习框架可以自动计算导数：我们首先将梯度附加到想要对其计算偏导数的变量上。然后我们记录目标值的计算，执行它的反向传播函数，并访问得到的梯度。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 为什么计算二阶导数比一阶导数的开销要更大？\n",
    "1. 在运行反向传播函数之后，立即再次运行它，看看会发生什么。\n",
    "1. 在控制流的例子中，我们计算`d`关于`a`的导数，如果我们将变量`a`更改为随机向量或矩阵，会发生什么？\n",
    "1. 重新设计一个求控制流梯度的例子，运行并分析结果。\n",
    "1. 使$f(x)=\\sin(x)$，绘制$f(x)$和$\\frac{df(x)}{dx}$的图像，其中后者不使用$f'(x)=\\cos(x)$。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a99037",
   "metadata": {
    "origin_pos": 50,
    "pycharm": {
     "name": "#%% md\n"
    },
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/1759)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
