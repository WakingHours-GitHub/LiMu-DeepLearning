{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "import tool_package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 实现一下LeNet. \n",
    "# 首先我们定义一下Reshape方法。\n",
    "class Reshape(torch.nn.Module):\n",
    "    def forward(self, X: nn.Module):\n",
    "        return X.view(-1, 1, 28, 28)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(), # LeNet使用Sigmoid()函数. \n",
    "    nn.AvgPool2d(kernel_size=2, stride=2), # 不重叠在一起. \n",
    "    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    # 做全连接层, 将输入进行平坦. \n",
    "    nn.Flatten(), # 做全连接层, 我们需要flatten. 形成一个二维, 好像全连接层, 矩阵乘积\n",
    "    # 开始全连接层: \n",
    "    nn.Linear(16 * 5 * 5 ,120), nn.Sigmoid(), # 输入需要算一下的, 也就是最后一层. '\n",
    "    nn.Linear(120, 84), nn.Sigmoid(),\n",
    "    nn.Linear(84, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (1): Sigmoid()\n",
      "  (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (4): Sigmoid()\n",
      "  (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (6): Flatten(start_dim=1, end_dim=-1)\n",
      "  (7): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (8): Sigmoid()\n",
      "  (9): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (10): Sigmoid()\n",
      "  (11): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source shape\t\t torch.Size([1, 1, 28, 28])\n",
      "Conv2d output shape \t torch.Size([1, 6, 28, 28])\n",
      "Sigmoid output shape \t torch.Size([1, 6, 28, 28])\n",
      "AvgPool2d output shape \t torch.Size([1, 6, 14, 14])\n",
      "Conv2d output shape \t torch.Size([1, 16, 10, 10])\n",
      "Sigmoid output shape \t torch.Size([1, 16, 10, 10])\n",
      "AvgPool2d output shape \t torch.Size([1, 16, 5, 5])\n",
      "Flatten output shape \t torch.Size([1, 400])\n",
      "Linear output shape \t torch.Size([1, 120])\n",
      "Sigmoid output shape \t torch.Size([1, 120])\n",
      "Linear output shape \t torch.Size([1, 84])\n",
      "Sigmoid output shape \t torch.Size([1, 84])\n",
      "Linear output shape \t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# 测试输出形状: \n",
    "X = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)\n",
    "print(\"source shape\\t\\t\", X.shape)\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, \"output shape \\t\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n那么, 模型我们可以看到的思路:\\n就是卷积是将输入变小变小, 然后变宽变宽, 变小是指我们的关注的细节变小, 变宽, 是指我们关注的空间pattern(模式)\\n每一个通道就是没一个空间的pattern. 所以通道是一直在增加的, 而高宽是一直在减小的. \\n不断地将空间信息压缩压缩变小,然后通道变多, 就是将压缩的信息抽取出来, 形成一种空间方式, 然后放到不同的通道中去识别.\\n然后MLP就是将所有这些信息拿出来, 最终通过全连接层很smooth(平滑)的将模型压缩到我们想要的输出. \\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "那么, 模型我们可以看到的思路:\n",
    "就是卷积是将输入变小变小, 然后变宽变宽, 变小是指我们的关注的细节变小, 变宽, 是指我们关注的空间pattern(模式)\n",
    "每一个通道就是没一个空间的pattern. 所以通道是一直在增加的, 而高宽是一直在减小的. \n",
    "不断地将空间信息压缩压缩变小,然后通道变多, 就是将压缩的信息抽取出来, 形成一种空间方式, 然后放到不同的通道中去识别.\n",
    "然后MLP就是将所有这些信息拿出来, 最终通过全连接层很smooth(平滑)的将模型压缩到我们想要的输出. \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNET在Fashion-MNIST数据集中的表现. \n",
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size) # loda data, 加载数据. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们要使用GPU了. \n",
    "# 使用GPU进行评估. \n",
    "\n",
    "def evaluate_accuracy_gpu(net, data_iter, device=None): # 使用GPU进行评估\n",
    "    \"\"\"use GPU to calcuate accuracy in test datasets\"\"\"\n",
    "    if isinstance(net, nn.Module): # 如果是nn.Module实现的,\n",
    "        # start module evaluate\n",
    "        net.eval()\n",
    "        if not device:\n",
    "            # if device is None. then use first parameter.device as all device.\n",
    "            device = next(iter(net.parameters())).device \n",
    "\n",
    "    # 生成累加器: \n",
    "    metric = d2l.Accumulator(3) # generate len is 3, Accumlator. \n",
    "    \n",
    "    # 评估: \n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            # 数据迁移. \n",
    "            if isinstance(X, list):\n",
    "                # BERT微调所需. 后面介绍?\n",
    "                X = [x.to(device) for x in X] \n",
    "            else:\n",
    "                X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
