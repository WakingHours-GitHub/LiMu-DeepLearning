{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/wakinghours/programming/LiMu-DeepLearning/kaggle/cifar10'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "import cv2 as cv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import *\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "from utils import Accumulator, train\n",
    "from net import resnet18\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "runtime_path = sys.path[0]\n",
    "runtime_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_all_GPUS() -> List[torch.device]:\n",
    "    devices = [torch.device(f\"cuda:{i}\") for i in range(torch.cuda.device_count())]\n",
    "    return devices if devices else torch.device(\"cpu\")\n",
    "\n",
    "devices = try_all_GPUS()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_join = lambda *args: os.path.join(*args)\n",
    "\n",
    "\n",
    "class CIFAR10_dataset(Dataset):\n",
    "    def __init__(self, type_dataset: str = \"train\", vaild_rate=0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.type_dataset = type_dataset\n",
    "        self.vaild_rate = vaild_rate\n",
    "        self.transform_train = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize(42),\n",
    "            transforms.RandomResizedCrop(32, (0.6, 1.0), ratio=(1.0, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.Normalize([0.4914, 0.4822, 0.4465],  # normalize. 归一化.\n",
    "                                 [0.2023, 0.1994, 0.2010])\n",
    "        ])\n",
    "        self.transform_vaild = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize([0.4914, 0.4822, 0.4465],  # normalize. 归一化.\n",
    "                                         [0.2023, 0.1994, 0.2010])\n",
    "                ])\n",
    "        self.labels_dict = self.parse_csv2label()\n",
    "        self.classes = ['airplane', 'automobile', 'bird', 'cat',\n",
    "                        'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "        self.root_path = path_join(runtime_path, \"train\")\n",
    "        if \"train\" == self.type_dataset:\n",
    "            # generate train and vaild dataset file.\n",
    "            self.shuffle_train_vaild()\n",
    "            # only generate in \"train\" mode.\n",
    "\n",
    "            with open(\"./train.txt\", \"r\") as f:\n",
    "                self.file_path_list = f.readlines()\n",
    "\n",
    "        elif \"vaild\" == self.type_dataset:\n",
    "            with open(\"./train_vaild.txt\", \"r\") as f:\n",
    "                self.file_path_list = f.readlines()\n",
    "        else: # test:\n",
    "            self.root_path = path_join(runtime_path, \"test\")\n",
    "\n",
    "            self.file_path_list = [path_join(self.root_path, file_name) for file_name in os.listdir(self.root_path)]\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_path = self.file_path_list[index].strip()\n",
    "        file_name = file_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        img = cv.imread(file_path)\n",
    "        if \"train\" == self.type_dataset:\n",
    "            X = self.transform_train(img)\n",
    "            return X, self.classes.index(self.labels_dict[file_name])\n",
    "        elif \"vaild\" == self.type_dataset:\n",
    "            X = self.transform_vaild(img)\n",
    "\n",
    "            return X, self.classes.index(self.labels_dict[file_name])\n",
    "\n",
    "        else: # test\n",
    "            X = self.transform_vaild(img)\n",
    "            return X, file_name\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def parse_csv2label(self):\n",
    "        with open(\"./trainLabels.csv\", \"r\") as f:\n",
    "            return {ele[0]: ele[1] for ele in [line.strip().split(',') for line in f.readlines()][1:]}\n",
    "\n",
    "    def shuffle_train_vaild(self):\n",
    "        l = len(os.listdir(self.root_path))\n",
    "\n",
    "        try:\n",
    "            file_name_list = os.listdir(self.root_path)\n",
    "            random.shuffle(file_name_list)\n",
    "\n",
    "            with open(path_join(self.root_path, \"../\", \"train.txt\"), \"w\") as train_file_writer:\n",
    "                train_file_writer.write(\n",
    "                    \"\\n\".join([path_join(self.root_path,  file_name)\n",
    "                              for file_name in file_name_list[0: int(l*(1-self.vaild_rate))]])\n",
    "                )\n",
    "\n",
    "            with open(path_join(self.root_path, \"../\",  \"train_vaild.txt\"), \"w\") as train_file_writer:\n",
    "                train_file_writer.write(\n",
    "                    \"\\n\".join([path_join(self.root_path,  file_name)\n",
    "                              for file_name in file_name_list[int(l*(1-self.vaild_rate)):]])\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"error: \", e)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_path_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_CIFAR10_iter(batch_size=64, num_workers=28):\n",
    "    return DataLoader(\n",
    "        CIFAR10_dataset(\"train\"),\n",
    "        batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    ),     DataLoader(\n",
    "        CIFAR10_dataset(\"vaild\"),\n",
    "        batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wakinghours/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/wakinghours/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# net = resnet18() # 实例化. \n",
    "import torchvision\n",
    "net = torchvision.models.resnet34(pretrained=True)\n",
    "\n",
    "# net = torchvision.models.resnet101(pretrained=True)\n",
    "\n",
    "epoch = 150\n",
    "batch_size = 512\n",
    "lr = 5e-3\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net.fc = nn.Linear(512, 10)\n",
    "print(net)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on:  [device(type='cuda', index=0), device(type='cuda', index=1)]\n",
      "10 test acc: 0.6993303537368775 train loss: 0.6332733068953861 train acc: 0.7847734906456687\n",
      "20 test acc: 0.7006815969944 train loss: 0.6478056887334044 train acc: 0.7805868874896656\n",
      "30 test acc: 0.6643734037876129 train loss: 0.6541716199029576 train acc: 0.7791890170086514\n",
      "40 test acc: 0.6612324595451355 train loss: 0.6507146839391101 train acc: 0.7795768386938355\n",
      "50 test acc: 0.7230070114135743 train loss: 0.6416514780033719 train acc: 0.7811024256727912\n",
      "60 test acc: 0.7278698980808258 train loss: 0.6492209190672095 train acc: 0.7787618684497747\n",
      "70 test acc: 0.7238600134849549 train loss: 0.6678034101020206 train acc: 0.7724936455488205\n",
      "80 test acc: 0.741497927904129 train loss: 0.6548780284144662 train acc: 0.7779091285033659\n",
      "90 test acc: 0.7174306452274323 train loss: 0.6594853814352643 train acc: 0.774488823657686\n",
      "100 test acc: 0.7041374325752259 train loss: 0.642893907698718 train acc: 0.7807944267988205\n",
      "110 test acc: 0.7467235326766968 train loss: 0.6081049228933725 train acc: 0.7932311912829225\n",
      "120 test acc: 0.7056839883327484 train loss: 0.5637991739944979 train acc: 0.807036233896559\n",
      "130 test acc: 0.7615951836109162 train loss: 0.5488918542184613 train acc: 0.8127180520783771\n",
      "140 test acc: 0.7491709172725678 train loss: 0.5256689248437231 train acc: 0.8218953257257288\n",
      "150 test acc: 0.7616868615150452 train loss: 0.4952834797176448 train acc: 0.8312135541980917\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_iter, vaild_iter = load_CIFAR10_iter(batch_size)\n",
    "\n",
    "train(\n",
    "    net,\n",
    "    loss_fn,\n",
    "    train_iter, vaild_iter, \n",
    "    lr, epoch, \n",
    "    80, 10, 0.9, 5e-3,\n",
    "    \"logs/epoch150_testacc0.78_loss0.49_acc0.83.pth\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'automobile',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'deer',\n",
       " 'dog',\n",
       " 'frog',\n",
       " 'horse',\n",
       " 'ship',\n",
       " 'truck']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_datasets = CIFAR10_dataset(\"test\")\n",
    "test_iter = DataLoader(\n",
    "    test_datasets,\n",
    "    100000,\n",
    "    shuffle=False,\n",
    "    num_workers=30,\n",
    ")\n",
    "\n",
    "classes = test_datasets.classes\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evel_test_iter(net, test_iter: DataLoader, load_path = None, devices=try_all_GPUS()):\n",
    "    net = torch.nn.DataParallel(net, devices).to(device=devices[0])\n",
    "    pred, id = [], []\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        # result = [(classes[net(features).argmax(dim=1).cpu().numpy()[0]], int(file_name[0])) for features, file_name in test_iter]\n",
    "        for features, file_name in test_iter:\n",
    "            pred.extend(list(net(features).argmax(dim=1).cpu().numpy()))\n",
    "            id.extend([int(ele) for ele in file_name])\n",
    "\n",
    "        # print(result[0])\n",
    "    \n",
    "    result = zip(id, pred)\n",
    "    retult = sorted(result, key=lambda x: x[0])\n",
    "    df = pd.DataFrame({'id': [tup[0] for tup in retult], 'label':  [tup[1] for tup in retult]})\n",
    "    df['label'] = df['label'].apply(lambda x: classes[x])\n",
    "    df.to_csv('./submission.csv', index=False)\n",
    "    \n",
    "\n",
    "evel_test_iter(net, test_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
