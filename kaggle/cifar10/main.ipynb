{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "import cv2 as cv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import *\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "from utils import *\n",
    "from net import resnet18\n",
    "import pandas as pd\n",
    "import math\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "\n",
    "runtime_path = sys.path[0]\n",
    "runtime_path\n",
    "\n",
    "os.chdir(runtime_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[device(type='cuda', index=0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def try_all_GPUS() -> List[torch.device]:\n",
    "    devices = [torch.device(f\"cuda:{i}\") for i in range(torch.cuda.device_count())]\n",
    "    return devices if devices else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "devices = try_all_GPUS()\n",
    "devices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init parameters used by xavier_uniform method. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "resnet18(\n",
       "  (stage1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (stage2): Sequential(\n",
       "    (0): Residual(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): Residual(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (stage3): Sequential(\n",
       "    (0): Residual(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
       "    )\n",
       "    (1): Residual(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (stage4): Sequential(\n",
       "    (0): Residual(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
       "    )\n",
       "    (1): Residual(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (stage5): Sequential(\n",
       "    (0): Residual(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
       "    )\n",
       "    (1): Residual(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (global_avg): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_join = lambda *args: os.path.join(*args)\n",
    "\n",
    "\n",
    "class CIFAR10_dataset(Dataset):\n",
    "    def __init__(self, type_dataset: str = \"train\", vaild_rate=0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.type_dataset = type_dataset\n",
    "        self.vaild_rate = vaild_rate\n",
    "        self.transform_train = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize(42),\n",
    "            transforms.RandomResizedCrop(32, (0.6, 1.0), ratio=(0.8, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(p=0.8),\n",
    "            transforms.Normalize([0.4914, 0.4822, 0.4465],  # normalize. 归一化.\n",
    "                                 [0.2023, 0.1994, 0.2010])\n",
    "        ])\n",
    "        self.transform_vaild = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize([0.4914, 0.4822, 0.4465],  # normalize. 归一化.\n",
    "                                         [0.2023, 0.1994, 0.2010])\n",
    "                ])\n",
    "        self.labels_dict = self.parse_csv2label()\n",
    "        self.classes = ['airplane', 'automobile', 'bird', 'cat',\n",
    "                        'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "        self.root_path = path_join(runtime_path, \"train\")\n",
    "        if \"train\" == self.type_dataset:\n",
    "            # generate train and vaild dataset file.\n",
    "            self.shuffle_train_vaild()\n",
    "            # only generate in \"train\" mode.\n",
    "\n",
    "            with open(\"./train.txt\", \"r\") as f:\n",
    "                self.file_path_list = f.readlines()\n",
    "\n",
    "        elif \"vaild\" == self.type_dataset:\n",
    "            with open(\"./train_vaild.txt\", \"r\") as f:\n",
    "                self.file_path_list = f.readlines()\n",
    "        else: # test:\n",
    "            self.root_path = path_join(runtime_path, \"test\")\n",
    "\n",
    "            self.file_path_list = [path_join(self.root_path, file_name) for file_name in os.listdir(self.root_path)]\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_path = self.file_path_list[index].strip()\n",
    "        file_name = file_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        img = cv.imread(file_path)\n",
    "        if \"train\" == self.type_dataset:\n",
    "            X = self.transform_train(img)\n",
    "            return X, self.classes.index(self.labels_dict[file_name])\n",
    "        elif \"vaild\" == self.type_dataset:\n",
    "            X = self.transform_vaild(img)\n",
    "\n",
    "            return X, self.classes.index(self.labels_dict[file_name])\n",
    "\n",
    "        else: # test\n",
    "            X = self.transform_vaild(img)\n",
    "            return X, file_name\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def parse_csv2label(self):\n",
    "        with open(\"./trainLabels.csv\", \"r\") as f:\n",
    "            return {ele[0]: ele[1] for ele in [line.strip().split(',') for line in f.readlines()][1:]}\n",
    "\n",
    "    def shuffle_train_vaild(self):\n",
    "        l = len(os.listdir(self.root_path))\n",
    "\n",
    "        try:\n",
    "            file_name_list = os.listdir(self.root_path)\n",
    "            random.shuffle(file_name_list)\n",
    "\n",
    "            with open(path_join(self.root_path, \"../\", \"train.txt\"), \"w\") as train_file_writer:\n",
    "                train_file_writer.write(\n",
    "                    \"\\n\".join([path_join(self.root_path,  file_name)\n",
    "                              for file_name in file_name_list[0: int(l*(1-self.vaild_rate))]])\n",
    "                )\n",
    "\n",
    "            with open(path_join(self.root_path, \"../\",  \"train_vaild.txt\"), \"w\") as train_file_writer:\n",
    "                train_file_writer.write(\n",
    "                    \"\\n\".join([path_join(self.root_path,  file_name)\n",
    "                              for file_name in file_name_list[int(l*(1-self.vaild_rate)):]])\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"error: \", e)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_path_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_CIFAR10_iter(batch_size=64, num_workers=28):\n",
    "    return DataLoader(\n",
    "        CIFAR10_dataset(\"train\"),\n",
    "        batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    ),     DataLoader(\n",
    "        CIFAR10_dataset(\"vaild\"),\n",
    "        batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "\n",
    "net = resnet18() # 实例化. \n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "# net = torchvision.models.resnet34(pretrained=True)\n",
    "\n",
    "# net = torchvision.models.resnet101(pretrained=True)\n",
    "\n",
    "epoch = 80\n",
    "batch_size = 512\n",
    "lr = 5e-2\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# train_cifar10(net, lr, batch_size, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/wakinghours/programming/LiMu-DeepLearning/kaggle/cifar10/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# net.fc = nn.Linear(512, 10)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# print(net);\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m train_iter, vaild_iter \u001b[39m=\u001b[39m load_CIFAR10_iter(batch_size)\n\u001b[1;32m      5\u001b[0m \u001b[39m# iter(train_iter).__next__()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 97\u001b[0m, in \u001b[0;36mload_CIFAR10_iter\u001b[0;34m(batch_size, num_workers)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_CIFAR10_iter\u001b[39m(batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, num_workers\u001b[39m=\u001b[39m\u001b[39m28\u001b[39m):\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m DataLoader(\n\u001b[0;32m---> 97\u001b[0m         CIFAR10_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m     98\u001b[0m         batch_size,\n\u001b[1;32m     99\u001b[0m         shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    100\u001b[0m         num_workers\u001b[39m=\u001b[39mnum_workers\n\u001b[1;32m    101\u001b[0m     ),     DataLoader(\n\u001b[1;32m    102\u001b[0m         CIFAR10_dataset(\u001b[39m\"\u001b[39m\u001b[39mvaild\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    103\u001b[0m         batch_size,\n\u001b[1;32m    104\u001b[0m         shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    105\u001b[0m         num_workers\u001b[39m=\u001b[39mnum_workers\n\u001b[1;32m    106\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[6], line 29\u001b[0m, in \u001b[0;36mCIFAR10_dataset.__init__\u001b[0;34m(self, type_dataset, vaild_rate)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_path \u001b[39m=\u001b[39m path_join(runtime_path, \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtype_dataset:\n\u001b[1;32m     28\u001b[0m     \u001b[39m# generate train and vaild dataset file.\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshuffle_train_vaild()\n\u001b[1;32m     30\u001b[0m     \u001b[39m# only generate in \"train\" mode.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m./train.txt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "Cell \u001b[0;32mIn[6], line 68\u001b[0m, in \u001b[0;36mCIFAR10_dataset.shuffle_train_vaild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshuffle_train_vaild\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 68\u001b[0m     l \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(os\u001b[39m.\u001b[39;49mlistdir(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot_path))\n\u001b[1;32m     70\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m         file_name_list \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mlistdir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_path)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/wakinghours/programming/LiMu-DeepLearning/kaggle/cifar10/train'"
     ]
    }
   ],
   "source": [
    "# net.fc = nn.Linear(512, 10)\n",
    "# print(net);\n",
    "\n",
    "train_iter, vaild_iter = load_CIFAR10_iter(batch_size)\n",
    "# iter(train_iter).__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on: [device(type='cuda', index=0), device(type='cuda', index=1)]\n",
      "10 test acc: 0.6994 train loss: 0.0013543997406959534 train acc: 0.7572\n",
      "20 test acc: 0.7828 train loss: 0.0008369606859154171 train acc: 0.8506666666666667\n",
      "30 test acc: 0.7912 train loss: 0.0005660829418235355 train acc: 0.8988\n",
      "40 test acc: 0.807 train loss: 0.00040100363426738317 train acc: 0.9293777777777777\n",
      "50 test acc: 0.8198 train loss: 0.00030241927206516267 train acc: 0.9480444444444445\n",
      "60 test acc: 0.8196 train loss: 0.0002213421877887514 train acc: 0.9624444444444444\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m                 os\u001b[39m.\u001b[39mmkdir(\u001b[39m\"\u001b[39m\u001b[39m./logs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m                 torch\u001b[39m.\u001b[39msave(net\u001b[39m.\u001b[39mstate_dict(), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./logs/epoch\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m_testacc\u001b[39m\u001b[39m{\u001b[39;00mtest_accuracy\u001b[39m:\u001b[39;00m\u001b[39m4.3\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m_loss\u001b[39m\u001b[39m{\u001b[39;00mmetric[\u001b[39m0\u001b[39m]\u001b[39m/\u001b[39mmetric[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m3.2\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m_acc\u001b[39m\u001b[39m{\u001b[39;00mmetric[\u001b[39m1\u001b[39m]\u001b[39m/\u001b[39mmetric[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.2\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.pth\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m train_cos(net, loss_fn, train_iter, vaild_iter, lr, epoch)\n\u001b[1;32m     57\u001b[0m \u001b[39m# train(\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39m#     net,\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m#     loss_fn,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39m#     # \"                                                                                    logs/epoch200_testacc0.823_loss0.29_acc0.9.pth\"\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39m# )\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 38\u001b[0m, in \u001b[0;36mtrain_cos\u001b[0;34m(net, loss_fn, train_iter, test_iter, lr, num_epoch, momentum, weight_decay, load_path, devices)\u001b[0m\n\u001b[1;32m     35\u001b[0m     loss\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mbackward() \u001b[39m# than calculate graient by backwoard (pro)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     trainer\u001b[39m.\u001b[39mstep() \u001b[39m# update weight. \u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     metric\u001b[39m.\u001b[39;49madd(loss\u001b[39m.\u001b[39;49mitem(), accuracy(y_hat, labels), labels\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m     41\u001b[0m scheduler\u001b[39m.\u001b[39mstep() \u001b[39m# we only update scheduler. \u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39m# evaluate\u001b[39;00m\n",
      "File \u001b[0;32m~/programming/LiMu-DeepLearning/kaggle/cifar10/utils.py:246\u001b[0m, in \u001b[0;36mAccumulator.add\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[0;32m--> 246\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m [a\u001b[39m+\u001b[39m\u001b[39mfloat\u001b[39m(b) \u001b[39mfor\u001b[39;00m a, b \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata, args)]\n",
      "File \u001b[0;32m~/programming/LiMu-DeepLearning/kaggle/cifar10/utils.py:246\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[0;32m--> 246\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m [a\u001b[39m+\u001b[39m\u001b[39mfloat\u001b[39;49m(b) \u001b[39mfor\u001b[39;00m a, b \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata, args)]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def train_cos(\n",
    "    net: nn.Module, loss_fn: nn.Module,\n",
    "    train_iter:DataLoader, test_iter:DataLoader,\n",
    "    lr, num_epoch,\n",
    "    momentum=0.937, weight_decay=5e-4,\n",
    "    load_path:str=None, devices=try_all_GPUS(),\n",
    "):\n",
    "    eps = 0.35\n",
    "    net = nn.DataParallel(net, devices).to(devices[0])\n",
    "    loss_fn.to(devices[0])\n",
    "    if load_path:\n",
    "        print(\"load net parameters: \", load_path)\n",
    "        net.load_state_dict(torch.load(load_path)) # load parameters for net module. \n",
    "    # trainer = torch.optim.SGD(net.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    lf = lambda x: ((1 + math.cos(x * math.pi / num_epoch)) / 2) * (1 - eps) + eps \n",
    "    scheduler = optim.lr_scheduler.LambdaLR(\n",
    "        optimizer=trainer, # 优化器.\n",
    "        lr_lambda=lf, # 学习率函数. \n",
    "    ) # 根据lambda自定义学习率. \n",
    "\n",
    "    metric = Accumulator(3)\n",
    "    trainer.zero_grad() # empty.\n",
    "    # train:\n",
    "    print(\"train on:\", devices)\n",
    "    for epoch in range(num_epoch):\n",
    "        net.train()\n",
    "        metric.reset() # 重置\n",
    "        # train a epoch. \n",
    "        for i, (x, labels) in enumerate(train_iter):\n",
    "            x, labels = x.to(devices[0]), labels.to(devices[0])\n",
    "            y_hat = net(x)\n",
    "            loss = loss_fn(y_hat, labels)\n",
    "\n",
    "            trainer.zero_grad() # first empty gradient\n",
    "            loss.sum().backward() # than calculate graient by backwoard (pro)\n",
    "            trainer.step() # update weight. \n",
    "        \n",
    "            metric.add(loss.sum(), accuracy(y_hat, labels), 1)\n",
    "        \n",
    "        scheduler.step() # we only update scheduler. \n",
    "\n",
    "        # evaluate\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            test_accuracy = evaluate_test_with_GPUS(net, test_iter)\n",
    "            print(epoch+1, \"test acc:\", test_accuracy, \"train loss:\", metric[0]/metric[-1], \"train acc:\", metric[1]/metric[-1])\n",
    "\n",
    "            try:\n",
    "                torch.save(net.state_dict(), f\"./logs/epoch{epoch+1}_testacc{test_accuracy:4.3}_loss{metric[0]/metric[-1]:3.2}_acc{metric[1]/metric[-1]:.2}.pth\")\n",
    "            except:\n",
    "                os.mkdir(\"./logs\")\n",
    "                torch.save(net.state_dict(), f\"./logs/epoch{epoch+1}_testacc{test_accuracy:4.3}_loss{metric[0]/metric[-1]:3.2}_acc{metric[1]/metric[-1]:.2}.pth\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_cos(net, loss_fn, train_iter, vaild_iter, lr, epoch)\n",
    "\n",
    "\n",
    "# train(\n",
    "#     net,\n",
    "#     loss_fn,\n",
    "#     train_iter, vaild_iter, \n",
    "#     lr, epoch, \n",
    "#     80, 10, 0.95, 8e-2,\n",
    "#     # \"                                                                                    logs/epoch200_testacc0.823_loss0.29_acc0.9.pth\"\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datasets = CIFAR10_dataset(\"test\")\n",
    "test_iter = DataLoader(\n",
    "    test_datasets,\n",
    "    50000,\n",
    "    shuffle=False,\n",
    "    num_workers=30,\n",
    ")\n",
    "\n",
    "classes = test_datasets.classes\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evel_test_iter(net, test_iter: DataLoader, load_path = None, devices=try_all_GPUS()):\n",
    "    net = torch.nn.DataParallel(net, devices).to(device=devices[0])\n",
    "    pred, id = [], []\n",
    "\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        # result = [(classes[net(features).argmax(dim=1).cpu().numpy()[0]], int(file_name[0])) for features, file_name in test_iter]\n",
    "        for features, file_name in test_iter:\n",
    "            pred.extend(list(net(features).argmax(dim=1).cpu().numpy()))\n",
    "            id.extend([int(ele) for ele in file_name])\n",
    "\n",
    "        # print(result[0])\n",
    "    \n",
    "    result = zip(id, pred)\n",
    "    retult = sorted(result, key=lambda x: x[0])\n",
    "    df = pd.DataFrame({'id': [tup[0] for tup in retult], 'label':  [tup[1] for tup in retult]})\n",
    "    df['label'] = df['label'].apply(lambda x: classes[x])\n",
    "    df.to_csv('./submission.csv', index=False)\n",
    "    \n",
    "\n",
    "evel_test_iter(net, test_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
