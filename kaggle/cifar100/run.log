nohup: 忽略输入
logs will save in:  ./runs/exp1
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 32, 112, 112]             864
       BatchNorm2d-2         [-1, 32, 112, 112]              64
              SiLU-3         [-1, 32, 112, 112]               0
            Conv2d-4         [-1, 32, 112, 112]             288
       BatchNorm2d-5         [-1, 32, 112, 112]              64
              SiLU-6         [-1, 32, 112, 112]               0
            Conv2d-7              [-1, 8, 1, 1]             264
              SiLU-8              [-1, 8, 1, 1]               0
            Conv2d-9             [-1, 32, 1, 1]             288
          Sigmoid-10             [-1, 32, 1, 1]               0
SqueezeExcitation-11         [-1, 32, 112, 112]               0
           Conv2d-12         [-1, 16, 112, 112]             512
      BatchNorm2d-13         [-1, 16, 112, 112]              32
         Identity-14         [-1, 16, 112, 112]               0
         Identity-15         [-1, 16, 112, 112]               0
 InvertedResidual-16         [-1, 16, 112, 112]               0
           Conv2d-17         [-1, 96, 112, 112]           1,536
      BatchNorm2d-18         [-1, 96, 112, 112]             192
             SiLU-19         [-1, 96, 112, 112]               0
           Conv2d-20           [-1, 96, 56, 56]             864
      BatchNorm2d-21           [-1, 96, 56, 56]             192
             SiLU-22           [-1, 96, 56, 56]               0
           Conv2d-23              [-1, 4, 1, 1]             388
             SiLU-24              [-1, 4, 1, 1]               0
           Conv2d-25             [-1, 96, 1, 1]             480
          Sigmoid-26             [-1, 96, 1, 1]               0
SqueezeExcitation-27           [-1, 96, 56, 56]               0
           Conv2d-28           [-1, 24, 56, 56]           2,304
      BatchNorm2d-29           [-1, 24, 56, 56]              48
         Identity-30           [-1, 24, 56, 56]               0
         Identity-31           [-1, 24, 56, 56]               0
 InvertedResidual-32           [-1, 24, 56, 56]               0
           Conv2d-33          [-1, 144, 56, 56]           3,456
      BatchNorm2d-34          [-1, 144, 56, 56]             288
             SiLU-35          [-1, 144, 56, 56]               0
           Conv2d-36          [-1, 144, 56, 56]           1,296
      BatchNorm2d-37          [-1, 144, 56, 56]             288
             SiLU-38          [-1, 144, 56, 56]               0
           Conv2d-39              [-1, 6, 1, 1]             870
             SiLU-40              [-1, 6, 1, 1]               0
           Conv2d-41            [-1, 144, 1, 1]           1,008
          Sigmoid-42            [-1, 144, 1, 1]               0
SqueezeExcitation-43          [-1, 144, 56, 56]               0
           Conv2d-44           [-1, 24, 56, 56]           3,456
      BatchNorm2d-45           [-1, 24, 56, 56]              48
         Identity-46           [-1, 24, 56, 56]               0
         DropPath-47           [-1, 24, 56, 56]               0
 InvertedResidual-48           [-1, 24, 56, 56]               0
           Conv2d-49          [-1, 144, 56, 56]           3,456
      BatchNorm2d-50          [-1, 144, 56, 56]             288
             SiLU-51          [-1, 144, 56, 56]               0
           Conv2d-52          [-1, 144, 28, 28]           3,600
      BatchNorm2d-53          [-1, 144, 28, 28]             288
             SiLU-54          [-1, 144, 28, 28]               0
           Conv2d-55              [-1, 6, 1, 1]             870
             SiLU-56              [-1, 6, 1, 1]               0
           Conv2d-57            [-1, 144, 1, 1]           1,008
          Sigmoid-58            [-1, 144, 1, 1]               0
SqueezeExcitation-59          [-1, 144, 28, 28]               0
           Conv2d-60           [-1, 40, 28, 28]           5,760
      BatchNorm2d-61           [-1, 40, 28, 28]              80
         Identity-62           [-1, 40, 28, 28]               0
         Identity-63           [-1, 40, 28, 28]               0
 InvertedResidual-64           [-1, 40, 28, 28]               0
           Conv2d-65          [-1, 240, 28, 28]           9,600
      BatchNorm2d-66          [-1, 240, 28, 28]             480
             SiLU-67          [-1, 240, 28, 28]               0
           Conv2d-68          [-1, 240, 28, 28]           6,000
      BatchNorm2d-69          [-1, 240, 28, 28]             480
             SiLU-70          [-1, 240, 28, 28]               0
           Conv2d-71             [-1, 10, 1, 1]           2,410
             SiLU-72             [-1, 10, 1, 1]               0
           Conv2d-73            [-1, 240, 1, 1]           2,640
          Sigmoid-74            [-1, 240, 1, 1]               0
SqueezeExcitation-75          [-1, 240, 28, 28]               0
           Conv2d-76           [-1, 40, 28, 28]           9,600
      BatchNorm2d-77           [-1, 40, 28, 28]              80
         Identity-78           [-1, 40, 28, 28]               0
         DropPath-79           [-1, 40, 28, 28]               0
 InvertedResidual-80           [-1, 40, 28, 28]               0
           Conv2d-81          [-1, 240, 28, 28]           9,600
      BatchNorm2d-82          [-1, 240, 28, 28]             480
             SiLU-83          [-1, 240, 28, 28]               0
           Conv2d-84          [-1, 240, 14, 14]           2,160
      BatchNorm2d-85          [-1, 240, 14, 14]             480
             SiLU-86          [-1, 240, 14, 14]               0
           Conv2d-87             [-1, 10, 1, 1]           2,410
             SiLU-88             [-1, 10, 1, 1]               0
           Conv2d-89            [-1, 240, 1, 1]           2,640
          Sigmoid-90            [-1, 240, 1, 1]               0
SqueezeExcitation-91          [-1, 240, 14, 14]               0
           Conv2d-92           [-1, 80, 14, 14]          19,200
      BatchNorm2d-93           [-1, 80, 14, 14]             160
         Identity-94           [-1, 80, 14, 14]               0
         Identity-95           [-1, 80, 14, 14]               0
 InvertedResidual-96           [-1, 80, 14, 14]               0
           Conv2d-97          [-1, 480, 14, 14]          38,400
      BatchNorm2d-98          [-1, 480, 14, 14]             960
             SiLU-99          [-1, 480, 14, 14]               0
          Conv2d-100          [-1, 480, 14, 14]           4,320
     BatchNorm2d-101          [-1, 480, 14, 14]             960
            SiLU-102          [-1, 480, 14, 14]               0
          Conv2d-103             [-1, 20, 1, 1]           9,620
            SiLU-104             [-1, 20, 1, 1]               0
          Conv2d-105            [-1, 480, 1, 1]          10,080
         Sigmoid-106            [-1, 480, 1, 1]               0
SqueezeExcitation-107          [-1, 480, 14, 14]               0
          Conv2d-108           [-1, 80, 14, 14]          38,400
     BatchNorm2d-109           [-1, 80, 14, 14]             160
        Identity-110           [-1, 80, 14, 14]               0
        DropPath-111           [-1, 80, 14, 14]               0
InvertedResidual-112           [-1, 80, 14, 14]               0
          Conv2d-113          [-1, 480, 14, 14]          38,400
     BatchNorm2d-114          [-1, 480, 14, 14]             960
            SiLU-115          [-1, 480, 14, 14]               0
          Conv2d-116          [-1, 480, 14, 14]           4,320
     BatchNorm2d-117          [-1, 480, 14, 14]             960
            SiLU-118          [-1, 480, 14, 14]               0
          Conv2d-119             [-1, 20, 1, 1]           9,620
            SiLU-120             [-1, 20, 1, 1]               0
          Conv2d-121            [-1, 480, 1, 1]          10,080
         Sigmoid-122            [-1, 480, 1, 1]               0
SqueezeExcitation-123          [-1, 480, 14, 14]               0
          Conv2d-124           [-1, 80, 14, 14]          38,400
     BatchNorm2d-125           [-1, 80, 14, 14]             160
        Identity-126           [-1, 80, 14, 14]               0
        DropPath-127           [-1, 80, 14, 14]               0
InvertedResidual-128           [-1, 80, 14, 14]               0
          Conv2d-129          [-1, 480, 14, 14]          38,400
     BatchNorm2d-130          [-1, 480, 14, 14]             960
            SiLU-131          [-1, 480, 14, 14]               0
          Conv2d-132          [-1, 480, 14, 14]          12,000
     BatchNorm2d-133          [-1, 480, 14, 14]             960
            SiLU-134          [-1, 480, 14, 14]               0
          Conv2d-135             [-1, 20, 1, 1]           9,620
            SiLU-136             [-1, 20, 1, 1]               0
          Conv2d-137            [-1, 480, 1, 1]          10,080
         Sigmoid-138            [-1, 480, 1, 1]               0
SqueezeExcitation-139          [-1, 480, 14, 14]               0
          Conv2d-140          [-1, 112, 14, 14]          53,760
     BatchNorm2d-141          [-1, 112, 14, 14]             224
        Identity-142          [-1, 112, 14, 14]               0
        Identity-143          [-1, 112, 14, 14]               0
InvertedResidual-144          [-1, 112, 14, 14]               0
          Conv2d-145          [-1, 672, 14, 14]          75,264
     BatchNorm2d-146          [-1, 672, 14, 14]           1,344
            SiLU-147          [-1, 672, 14, 14]               0
          Conv2d-148          [-1, 672, 14, 14]          16,800
     BatchNorm2d-149          [-1, 672, 14, 14]           1,344
            SiLU-150          [-1, 672, 14, 14]               0
          Conv2d-151             [-1, 28, 1, 1]          18,844
            SiLU-152             [-1, 28, 1, 1]               0
          Conv2d-153            [-1, 672, 1, 1]          19,488
         Sigmoid-154            [-1, 672, 1, 1]               0
SqueezeExcitation-155          [-1, 672, 14, 14]               0
          Conv2d-156          [-1, 112, 14, 14]          75,264
     BatchNorm2d-157          [-1, 112, 14, 14]             224
        Identity-158          [-1, 112, 14, 14]               0
        DropPath-159          [-1, 112, 14, 14]               0
InvertedResidual-160          [-1, 112, 14, 14]               0
          Conv2d-161          [-1, 672, 14, 14]          75,264
     BatchNorm2d-162          [-1, 672, 14, 14]           1,344
            SiLU-163          [-1, 672, 14, 14]               0
          Conv2d-164          [-1, 672, 14, 14]          16,800
     BatchNorm2d-165          [-1, 672, 14, 14]           1,344
            SiLU-166          [-1, 672, 14, 14]               0
          Conv2d-167             [-1, 28, 1, 1]          18,844
            SiLU-168             [-1, 28, 1, 1]               0
          Conv2d-169            [-1, 672, 1, 1]          19,488
         Sigmoid-170            [-1, 672, 1, 1]               0
SqueezeExcitation-171          [-1, 672, 14, 14]               0
          Conv2d-172          [-1, 112, 14, 14]          75,264
     BatchNorm2d-173          [-1, 112, 14, 14]             224
        Identity-174          [-1, 112, 14, 14]               0
        DropPath-175          [-1, 112, 14, 14]               0
InvertedResidual-176          [-1, 112, 14, 14]               0
          Conv2d-177          [-1, 672, 14, 14]          75,264
     BatchNorm2d-178          [-1, 672, 14, 14]           1,344
            SiLU-179          [-1, 672, 14, 14]               0
          Conv2d-180            [-1, 672, 7, 7]          16,800
     BatchNorm2d-181            [-1, 672, 7, 7]           1,344
            SiLU-182            [-1, 672, 7, 7]               0
          Conv2d-183             [-1, 28, 1, 1]          18,844
            SiLU-184             [-1, 28, 1, 1]               0
          Conv2d-185            [-1, 672, 1, 1]          19,488
         Sigmoid-186            [-1, 672, 1, 1]               0
SqueezeExcitation-187            [-1, 672, 7, 7]               0
          Conv2d-188            [-1, 192, 7, 7]         129,024
     BatchNorm2d-189            [-1, 192, 7, 7]             384
        Identity-190            [-1, 192, 7, 7]               0
        Identity-191            [-1, 192, 7, 7]               0
InvertedResidual-192            [-1, 192, 7, 7]               0
          Conv2d-193           [-1, 1152, 7, 7]         221,184
     BatchNorm2d-194           [-1, 1152, 7, 7]           2,304
            SiLU-195           [-1, 1152, 7, 7]               0
          Conv2d-196           [-1, 1152, 7, 7]          28,800
     BatchNorm2d-197           [-1, 1152, 7, 7]           2,304
            SiLU-198           [-1, 1152, 7, 7]               0
          Conv2d-199             [-1, 48, 1, 1]          55,344
            SiLU-200             [-1, 48, 1, 1]               0
          Conv2d-201           [-1, 1152, 1, 1]          56,448
         Sigmoid-202           [-1, 1152, 1, 1]               0
SqueezeExcitation-203           [-1, 1152, 7, 7]               0
          Conv2d-204            [-1, 192, 7, 7]         221,184
     BatchNorm2d-205            [-1, 192, 7, 7]             384
        Identity-206            [-1, 192, 7, 7]               0
        DropPath-207            [-1, 192, 7, 7]               0
InvertedResidual-208            [-1, 192, 7, 7]               0
          Conv2d-209           [-1, 1152, 7, 7]         221,184
     BatchNorm2d-210           [-1, 1152, 7, 7]           2,304
            SiLU-211           [-1, 1152, 7, 7]               0
          Conv2d-212           [-1, 1152, 7, 7]          28,800
     BatchNorm2d-213           [-1, 1152, 7, 7]           2,304
            SiLU-214           [-1, 1152, 7, 7]               0
          Conv2d-215             [-1, 48, 1, 1]          55,344
            SiLU-216             [-1, 48, 1, 1]               0
          Conv2d-217           [-1, 1152, 1, 1]          56,448
         Sigmoid-218           [-1, 1152, 1, 1]               0
SqueezeExcitation-219           [-1, 1152, 7, 7]               0
          Conv2d-220            [-1, 192, 7, 7]         221,184
     BatchNorm2d-221            [-1, 192, 7, 7]             384
        Identity-222            [-1, 192, 7, 7]               0
        DropPath-223            [-1, 192, 7, 7]               0
InvertedResidual-224            [-1, 192, 7, 7]               0
          Conv2d-225           [-1, 1152, 7, 7]         221,184
     BatchNorm2d-226           [-1, 1152, 7, 7]           2,304
            SiLU-227           [-1, 1152, 7, 7]               0
          Conv2d-228           [-1, 1152, 7, 7]          28,800
     BatchNorm2d-229           [-1, 1152, 7, 7]           2,304
            SiLU-230           [-1, 1152, 7, 7]               0
          Conv2d-231             [-1, 48, 1, 1]          55,344
            SiLU-232             [-1, 48, 1, 1]               0
          Conv2d-233           [-1, 1152, 1, 1]          56,448
         Sigmoid-234           [-1, 1152, 1, 1]               0
SqueezeExcitation-235           [-1, 1152, 7, 7]               0
          Conv2d-236            [-1, 192, 7, 7]         221,184
     BatchNorm2d-237            [-1, 192, 7, 7]             384
        Identity-238            [-1, 192, 7, 7]               0
        DropPath-239            [-1, 192, 7, 7]               0
InvertedResidual-240            [-1, 192, 7, 7]               0
          Conv2d-241           [-1, 1152, 7, 7]         221,184
     BatchNorm2d-242           [-1, 1152, 7, 7]           2,304
            SiLU-243           [-1, 1152, 7, 7]               0
          Conv2d-244           [-1, 1152, 7, 7]          10,368
     BatchNorm2d-245           [-1, 1152, 7, 7]           2,304
            SiLU-246           [-1, 1152, 7, 7]               0
          Conv2d-247             [-1, 48, 1, 1]          55,344
            SiLU-248             [-1, 48, 1, 1]               0
          Conv2d-249           [-1, 1152, 1, 1]          56,448
         Sigmoid-250           [-1, 1152, 1, 1]               0
SqueezeExcitation-251           [-1, 1152, 7, 7]               0
          Conv2d-252            [-1, 320, 7, 7]         368,640
     BatchNorm2d-253            [-1, 320, 7, 7]             640
        Identity-254            [-1, 320, 7, 7]               0
        Identity-255            [-1, 320, 7, 7]               0
InvertedResidual-256            [-1, 320, 7, 7]               0
          Conv2d-257           [-1, 1280, 7, 7]         409,600
     BatchNorm2d-258           [-1, 1280, 7, 7]           2,560
            SiLU-259           [-1, 1280, 7, 7]               0
AdaptiveAvgPool2d-260           [-1, 1280, 1, 1]               0
         Dropout-261                 [-1, 1280]               0
          Linear-262                  [-1, 100]         128,100
    EfficientNet-263                  [-1, 100]               0
================================================================
Total params: 4,135,648
Trainable params: 4,135,648
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.57
Forward/backward pass size (MB): 180.83
Params size (MB): 15.78
Estimated Total Size (MB): 197.18
----------------------------------------------------------------
train on: [device(type='cuda', index=0)]
save mode:  best
1 test acc: 0.01 train loss: 0.03624963975906372 train acc: 0.00886
best:  0.01
2 test acc: 0.01 train loss: 0.03615241269111633 train acc: 0.00918
3 test acc: 0.01 train loss: 0.03608689342498779 train acc: 0.00904
4 test acc: 0.01 train loss: 0.03605121155738831 train acc: 0.0087
5 test acc: 0.01 train loss: 0.03603535228729248 train acc: 0.00912
6 test acc: 0.01 train loss: 0.036027384719848635 train acc: 0.00946
7 test acc: 0.01 train loss: 0.036026024150848386 train acc: 0.00884
8 test acc: 0.01 train loss: 0.036022920427322386 train acc: 0.00918
9 test acc: 0.01 train loss: 0.03601949447631836 train acc: 0.00952
10 test acc: 0.0178 train loss: 0.035926266708374024 train acc: 0.01276
best:  0.0178
11 test acc: 0.0221 train loss: 0.03580070785522461 train acc: 0.01418
best:  0.0221
12 test acc: 0.0247 train loss: 0.03564058420181274 train acc: 0.01758
best:  0.0247
13 test acc: 0.027 train loss: 0.03544943148612976 train acc: 0.02048
best:  0.027
14 test acc: 0.0277 train loss: 0.03528784519195557 train acc: 0.02388
best:  0.0277
15 test acc: 0.0384 train loss: 0.03509249618530273 train acc: 0.02786
best:  0.0384
16 test acc: 0.0342 train loss: 0.034905833625793456 train acc: 0.0314
17 test acc: 0.0486 train loss: 0.034751877517700196 train acc: 0.03296
best:  0.0486
18 test acc: 0.0589 train loss: 0.034632394371032715 train acc: 0.03678
best:  0.0589
19 test acc: 0.059 train loss: 0.03451608019828797 train acc: 0.03864
best:  0.059
20 test acc: 0.0605 train loss: 0.03434373471260071 train acc: 0.04142
best:  0.0605
21 test acc: 0.0734 train loss: 0.034266001844406126 train acc: 0.04364
best:  0.0734
22 test acc: 0.0765 train loss: 0.034054214515686035 train acc: 0.0469
best:  0.0765
23 test acc: 0.0818 train loss: 0.033864985122680664 train acc: 0.04938
best:  0.0818
24 test acc: 0.0885 train loss: 0.03372271201133728 train acc: 0.05146
best:  0.0885
25 test acc: 0.0987 train loss: 0.03360151865959168 train acc: 0.05406
best:  0.0987
26 test acc: 0.0899 train loss: 0.03353374000549316 train acc: 0.05646
27 test acc: 0.1049 train loss: 0.033375506982803345 train acc: 0.05762
best:  0.1049
28 test acc: 0.1062 train loss: 0.0332203008556366 train acc: 0.06272
best:  0.1062
29 test acc: 0.1121 train loss: 0.03318371138572693 train acc: 0.06212
best:  0.1121
30 test acc: 0.1163 train loss: 0.033061573615074155 train acc: 0.06338
best:  0.1163
31 test acc: 0.1212 train loss: 0.03301914522647858 train acc: 0.0638
best:  0.1212
32 test acc: 0.1383 train loss: 0.03289781526088715 train acc: 0.0671
best:  0.1383
33 test acc: 0.128 train loss: 0.0328273295545578 train acc: 0.06788
34 test acc: 0.13 train loss: 0.03280818287372589 train acc: 0.06984
35 test acc: 0.1377 train loss: 0.03273108907222748 train acc: 0.06986
36 test acc: 0.1425 train loss: 0.03259802937984466 train acc: 0.0716
best:  0.1425
37 test acc: 0.1341 train loss: 0.03258913575649262 train acc: 0.07244
38 test acc: 0.1455 train loss: 0.03258502111434936 train acc: 0.07382
best:  0.1455
39 test acc: 0.1466 train loss: 0.03246288589477539 train acc: 0.07592
best:  0.1466
40 test acc: 0.1491 train loss: 0.032379470176696776 train acc: 0.07676
best:  0.1491
41 test acc: 0.1391 train loss: 0.03235114057064056 train acc: 0.07762
