nohup: 忽略输入
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 32, 112, 112]             864
       BatchNorm2d-2         [-1, 32, 112, 112]              64
              SiLU-3         [-1, 32, 112, 112]               0
            Conv2d-4         [-1, 32, 112, 112]             288
       BatchNorm2d-5         [-1, 32, 112, 112]              64
              SiLU-6         [-1, 32, 112, 112]               0
            Conv2d-7              [-1, 8, 1, 1]             264
              SiLU-8              [-1, 8, 1, 1]               0
            Conv2d-9             [-1, 32, 1, 1]             288
          Sigmoid-10             [-1, 32, 1, 1]               0
SqueezeExcitation-11         [-1, 32, 112, 112]               0
           Conv2d-12         [-1, 16, 112, 112]             512
      BatchNorm2d-13         [-1, 16, 112, 112]              32
         Identity-14         [-1, 16, 112, 112]               0
         Identity-15         [-1, 16, 112, 112]               0
 InvertedResidual-16         [-1, 16, 112, 112]               0
           Conv2d-17         [-1, 96, 112, 112]           1,536
      BatchNorm2d-18         [-1, 96, 112, 112]             192
             SiLU-19         [-1, 96, 112, 112]               0
           Conv2d-20           [-1, 96, 56, 56]             864
      BatchNorm2d-21           [-1, 96, 56, 56]             192
             SiLU-22           [-1, 96, 56, 56]               0
           Conv2d-23              [-1, 4, 1, 1]             388
             SiLU-24              [-1, 4, 1, 1]               0
           Conv2d-25             [-1, 96, 1, 1]             480
          Sigmoid-26             [-1, 96, 1, 1]               0
SqueezeExcitation-27           [-1, 96, 56, 56]               0
           Conv2d-28           [-1, 24, 56, 56]           2,304
      BatchNorm2d-29           [-1, 24, 56, 56]              48
         Identity-30           [-1, 24, 56, 56]               0
         Identity-31           [-1, 24, 56, 56]               0
 InvertedResidual-32           [-1, 24, 56, 56]               0
           Conv2d-33          [-1, 144, 56, 56]           3,456
      BatchNorm2d-34          [-1, 144, 56, 56]             288
             SiLU-35          [-1, 144, 56, 56]               0
           Conv2d-36          [-1, 144, 56, 56]           1,296
      BatchNorm2d-37          [-1, 144, 56, 56]             288
             SiLU-38          [-1, 144, 56, 56]               0
           Conv2d-39              [-1, 6, 1, 1]             870
             SiLU-40              [-1, 6, 1, 1]               0
           Conv2d-41            [-1, 144, 1, 1]           1,008
          Sigmoid-42            [-1, 144, 1, 1]               0
SqueezeExcitation-43          [-1, 144, 56, 56]               0
           Conv2d-44           [-1, 24, 56, 56]           3,456
      BatchNorm2d-45           [-1, 24, 56, 56]              48
         Identity-46           [-1, 24, 56, 56]               0
         DropPath-47           [-1, 24, 56, 56]               0
 InvertedResidual-48           [-1, 24, 56, 56]               0
           Conv2d-49          [-1, 144, 56, 56]           3,456
      BatchNorm2d-50          [-1, 144, 56, 56]             288
             SiLU-51          [-1, 144, 56, 56]               0
           Conv2d-52          [-1, 144, 28, 28]           3,600
      BatchNorm2d-53          [-1, 144, 28, 28]             288
             SiLU-54          [-1, 144, 28, 28]               0
           Conv2d-55              [-1, 6, 1, 1]             870
             SiLU-56              [-1, 6, 1, 1]               0
           Conv2d-57            [-1, 144, 1, 1]           1,008
          Sigmoid-58            [-1, 144, 1, 1]               0
SqueezeExcitation-59          [-1, 144, 28, 28]               0
           Conv2d-60           [-1, 40, 28, 28]           5,760
      BatchNorm2d-61           [-1, 40, 28, 28]              80
         Identity-62           [-1, 40, 28, 28]               0
         Identity-63           [-1, 40, 28, 28]               0
 InvertedResidual-64           [-1, 40, 28, 28]               0
           Conv2d-65          [-1, 240, 28, 28]           9,600
      BatchNorm2d-66          [-1, 240, 28, 28]             480
             SiLU-67          [-1, 240, 28, 28]               0
           Conv2d-68          [-1, 240, 28, 28]           6,000
      BatchNorm2d-69          [-1, 240, 28, 28]             480
             SiLU-70          [-1, 240, 28, 28]               0
           Conv2d-71             [-1, 10, 1, 1]           2,410
             SiLU-72             [-1, 10, 1, 1]               0
           Conv2d-73            [-1, 240, 1, 1]           2,640
          Sigmoid-74            [-1, 240, 1, 1]               0
SqueezeExcitation-75          [-1, 240, 28, 28]               0
           Conv2d-76           [-1, 40, 28, 28]           9,600
      BatchNorm2d-77           [-1, 40, 28, 28]              80
         Identity-78           [-1, 40, 28, 28]               0
         DropPath-79           [-1, 40, 28, 28]               0
 InvertedResidual-80           [-1, 40, 28, 28]               0
           Conv2d-81          [-1, 240, 28, 28]           9,600
      BatchNorm2d-82          [-1, 240, 28, 28]             480
             SiLU-83          [-1, 240, 28, 28]               0
           Conv2d-84          [-1, 240, 14, 14]           2,160
      BatchNorm2d-85          [-1, 240, 14, 14]             480
             SiLU-86          [-1, 240, 14, 14]               0
           Conv2d-87             [-1, 10, 1, 1]           2,410
             SiLU-88             [-1, 10, 1, 1]               0
           Conv2d-89            [-1, 240, 1, 1]           2,640
          Sigmoid-90            [-1, 240, 1, 1]               0
SqueezeExcitation-91          [-1, 240, 14, 14]               0
           Conv2d-92           [-1, 80, 14, 14]          19,200
      BatchNorm2d-93           [-1, 80, 14, 14]             160
         Identity-94           [-1, 80, 14, 14]               0
         Identity-95           [-1, 80, 14, 14]               0
 InvertedResidual-96           [-1, 80, 14, 14]               0
           Conv2d-97          [-1, 480, 14, 14]          38,400
      BatchNorm2d-98          [-1, 480, 14, 14]             960
             SiLU-99          [-1, 480, 14, 14]               0
          Conv2d-100          [-1, 480, 14, 14]           4,320
     BatchNorm2d-101          [-1, 480, 14, 14]             960
            SiLU-102          [-1, 480, 14, 14]               0
          Conv2d-103             [-1, 20, 1, 1]           9,620
            SiLU-104             [-1, 20, 1, 1]               0
          Conv2d-105            [-1, 480, 1, 1]          10,080
         Sigmoid-106            [-1, 480, 1, 1]               0
SqueezeExcitation-107          [-1, 480, 14, 14]               0
          Conv2d-108           [-1, 80, 14, 14]          38,400
     BatchNorm2d-109           [-1, 80, 14, 14]             160
        Identity-110           [-1, 80, 14, 14]               0
        DropPath-111           [-1, 80, 14, 14]               0
InvertedResidual-112           [-1, 80, 14, 14]               0
          Conv2d-113          [-1, 480, 14, 14]          38,400
     BatchNorm2d-114          [-1, 480, 14, 14]             960
            SiLU-115          [-1, 480, 14, 14]               0
          Conv2d-116          [-1, 480, 14, 14]           4,320
     BatchNorm2d-117          [-1, 480, 14, 14]             960
            SiLU-118          [-1, 480, 14, 14]               0
          Conv2d-119             [-1, 20, 1, 1]           9,620
            SiLU-120             [-1, 20, 1, 1]               0
          Conv2d-121            [-1, 480, 1, 1]          10,080
         Sigmoid-122            [-1, 480, 1, 1]               0
SqueezeExcitation-123          [-1, 480, 14, 14]               0
          Conv2d-124           [-1, 80, 14, 14]          38,400
     BatchNorm2d-125           [-1, 80, 14, 14]             160
        Identity-126           [-1, 80, 14, 14]               0
        DropPath-127           [-1, 80, 14, 14]               0
InvertedResidual-128           [-1, 80, 14, 14]               0
          Conv2d-129          [-1, 480, 14, 14]          38,400
     BatchNorm2d-130          [-1, 480, 14, 14]             960
            SiLU-131          [-1, 480, 14, 14]               0
          Conv2d-132          [-1, 480, 14, 14]          12,000
     BatchNorm2d-133          [-1, 480, 14, 14]             960
            SiLU-134          [-1, 480, 14, 14]               0
          Conv2d-135             [-1, 20, 1, 1]           9,620
            SiLU-136             [-1, 20, 1, 1]               0
          Conv2d-137            [-1, 480, 1, 1]          10,080
         Sigmoid-138            [-1, 480, 1, 1]               0
SqueezeExcitation-139          [-1, 480, 14, 14]               0
          Conv2d-140          [-1, 112, 14, 14]          53,760
     BatchNorm2d-141          [-1, 112, 14, 14]             224
        Identity-142          [-1, 112, 14, 14]               0
        Identity-143          [-1, 112, 14, 14]               0
InvertedResidual-144          [-1, 112, 14, 14]               0
          Conv2d-145          [-1, 672, 14, 14]          75,264
     BatchNorm2d-146          [-1, 672, 14, 14]           1,344
            SiLU-147          [-1, 672, 14, 14]               0
          Conv2d-148          [-1, 672, 14, 14]          16,800
     BatchNorm2d-149          [-1, 672, 14, 14]           1,344
            SiLU-150          [-1, 672, 14, 14]               0
          Conv2d-151             [-1, 28, 1, 1]          18,844
            SiLU-152             [-1, 28, 1, 1]               0
          Conv2d-153            [-1, 672, 1, 1]          19,488
         Sigmoid-154            [-1, 672, 1, 1]               0
SqueezeExcitation-155          [-1, 672, 14, 14]               0
          Conv2d-156          [-1, 112, 14, 14]          75,264
     BatchNorm2d-157          [-1, 112, 14, 14]             224
        Identity-158          [-1, 112, 14, 14]               0
        DropPath-159          [-1, 112, 14, 14]               0
InvertedResidual-160          [-1, 112, 14, 14]               0
          Conv2d-161          [-1, 672, 14, 14]          75,264
     BatchNorm2d-162          [-1, 672, 14, 14]           1,344
            SiLU-163          [-1, 672, 14, 14]               0
          Conv2d-164          [-1, 672, 14, 14]          16,800
     BatchNorm2d-165          [-1, 672, 14, 14]           1,344
            SiLU-166          [-1, 672, 14, 14]               0
          Conv2d-167             [-1, 28, 1, 1]          18,844
            SiLU-168             [-1, 28, 1, 1]               0
          Conv2d-169            [-1, 672, 1, 1]          19,488
         Sigmoid-170            [-1, 672, 1, 1]               0
SqueezeExcitation-171          [-1, 672, 14, 14]               0
          Conv2d-172          [-1, 112, 14, 14]          75,264
     BatchNorm2d-173          [-1, 112, 14, 14]             224
        Identity-174          [-1, 112, 14, 14]               0
        DropPath-175          [-1, 112, 14, 14]               0
InvertedResidual-176          [-1, 112, 14, 14]               0
          Conv2d-177          [-1, 672, 14, 14]          75,264
     BatchNorm2d-178          [-1, 672, 14, 14]           1,344
            SiLU-179          [-1, 672, 14, 14]               0
          Conv2d-180            [-1, 672, 7, 7]          16,800
     BatchNorm2d-181            [-1, 672, 7, 7]           1,344
            SiLU-182            [-1, 672, 7, 7]               0
          Conv2d-183             [-1, 28, 1, 1]          18,844
            SiLU-184             [-1, 28, 1, 1]               0
          Conv2d-185            [-1, 672, 1, 1]          19,488
         Sigmoid-186            [-1, 672, 1, 1]               0
SqueezeExcitation-187            [-1, 672, 7, 7]               0
          Conv2d-188            [-1, 192, 7, 7]         129,024
     BatchNorm2d-189            [-1, 192, 7, 7]             384
        Identity-190            [-1, 192, 7, 7]               0
        Identity-191            [-1, 192, 7, 7]               0
InvertedResidual-192            [-1, 192, 7, 7]               0
          Conv2d-193           [-1, 1152, 7, 7]         221,184
     BatchNorm2d-194           [-1, 1152, 7, 7]           2,304
            SiLU-195           [-1, 1152, 7, 7]               0
          Conv2d-196           [-1, 1152, 7, 7]          28,800
     BatchNorm2d-197           [-1, 1152, 7, 7]           2,304
            SiLU-198           [-1, 1152, 7, 7]               0
          Conv2d-199             [-1, 48, 1, 1]          55,344
            SiLU-200             [-1, 48, 1, 1]               0
          Conv2d-201           [-1, 1152, 1, 1]          56,448
         Sigmoid-202           [-1, 1152, 1, 1]               0
SqueezeExcitation-203           [-1, 1152, 7, 7]               0
          Conv2d-204            [-1, 192, 7, 7]         221,184
     BatchNorm2d-205            [-1, 192, 7, 7]             384
        Identity-206            [-1, 192, 7, 7]               0
        DropPath-207            [-1, 192, 7, 7]               0
InvertedResidual-208            [-1, 192, 7, 7]               0
          Conv2d-209           [-1, 1152, 7, 7]         221,184
     BatchNorm2d-210           [-1, 1152, 7, 7]           2,304
            SiLU-211           [-1, 1152, 7, 7]               0
          Conv2d-212           [-1, 1152, 7, 7]          28,800
     BatchNorm2d-213           [-1, 1152, 7, 7]           2,304
            SiLU-214           [-1, 1152, 7, 7]               0
          Conv2d-215             [-1, 48, 1, 1]          55,344
            SiLU-216             [-1, 48, 1, 1]               0
          Conv2d-217           [-1, 1152, 1, 1]          56,448
         Sigmoid-218           [-1, 1152, 1, 1]               0
SqueezeExcitation-219           [-1, 1152, 7, 7]               0
          Conv2d-220            [-1, 192, 7, 7]         221,184
     BatchNorm2d-221            [-1, 192, 7, 7]             384
        Identity-222            [-1, 192, 7, 7]               0
        DropPath-223            [-1, 192, 7, 7]               0
InvertedResidual-224            [-1, 192, 7, 7]               0
          Conv2d-225           [-1, 1152, 7, 7]         221,184
     BatchNorm2d-226           [-1, 1152, 7, 7]           2,304
            SiLU-227           [-1, 1152, 7, 7]               0
          Conv2d-228           [-1, 1152, 7, 7]          28,800
     BatchNorm2d-229           [-1, 1152, 7, 7]           2,304
            SiLU-230           [-1, 1152, 7, 7]               0
          Conv2d-231             [-1, 48, 1, 1]          55,344
            SiLU-232             [-1, 48, 1, 1]               0
          Conv2d-233           [-1, 1152, 1, 1]          56,448
         Sigmoid-234           [-1, 1152, 1, 1]               0
SqueezeExcitation-235           [-1, 1152, 7, 7]               0
          Conv2d-236            [-1, 192, 7, 7]         221,184
     BatchNorm2d-237            [-1, 192, 7, 7]             384
        Identity-238            [-1, 192, 7, 7]               0
        DropPath-239            [-1, 192, 7, 7]               0
InvertedResidual-240            [-1, 192, 7, 7]               0
          Conv2d-241           [-1, 1152, 7, 7]         221,184
     BatchNorm2d-242           [-1, 1152, 7, 7]           2,304
            SiLU-243           [-1, 1152, 7, 7]               0
          Conv2d-244           [-1, 1152, 7, 7]          10,368
     BatchNorm2d-245           [-1, 1152, 7, 7]           2,304
            SiLU-246           [-1, 1152, 7, 7]               0
          Conv2d-247             [-1, 48, 1, 1]          55,344
            SiLU-248             [-1, 48, 1, 1]               0
          Conv2d-249           [-1, 1152, 1, 1]          56,448
         Sigmoid-250           [-1, 1152, 1, 1]               0
SqueezeExcitation-251           [-1, 1152, 7, 7]               0
          Conv2d-252            [-1, 320, 7, 7]         368,640
     BatchNorm2d-253            [-1, 320, 7, 7]             640
        Identity-254            [-1, 320, 7, 7]               0
        Identity-255            [-1, 320, 7, 7]               0
InvertedResidual-256            [-1, 320, 7, 7]               0
          Conv2d-257           [-1, 1280, 7, 7]         409,600
     BatchNorm2d-258           [-1, 1280, 7, 7]           2,560
            SiLU-259           [-1, 1280, 7, 7]               0
AdaptiveAvgPool2d-260           [-1, 1280, 1, 1]               0
         Dropout-261                 [-1, 1280]               0
          Linear-262                   [-1, 10]          12,810
    EfficientNet-263                   [-1, 10]               0
================================================================
Total params: 4,020,358
Trainable params: 4,020,358
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.57
Forward/backward pass size (MB): 180.83
Params size (MB): 15.34
Estimated Total Size (MB): 196.74
----------------------------------------------------------------
train on: [device(type='cuda', index=0)]
save mode:  best
1 test acc: 0.3789 train loss: 0.03195122089624405 train acc: 0.23746
best:  0.3789
2 test acc: 0.6304 train loss: 0.02150087375998497 train acc: 0.50778
best:  0.6304
3 test acc: 0.7252 train loss: 0.015432246308326722 train acc: 0.65862
best:  0.7252
4 test acc: 0.7647 train loss: 0.012821066768169404 train acc: 0.71664
best:  0.7647
5 test acc: 0.7957 train loss: 0.011386060943603516 train acc: 0.75042
best:  0.7957
6 test acc: 0.8077 train loss: 0.010316721932291984 train acc: 0.7712
best:  0.8077
7 test acc: 0.7767 train loss: 0.009688615227937698 train acc: 0.78564
8 test acc: 0.8186 train loss: 0.009128445605635643 train acc: 0.798
best:  0.8186
9 test acc: 0.8272 train loss: 0.008682512196004391 train acc: 0.80884
best:  0.8272
10 test acc: 0.8225 train loss: 0.008282412012815476 train acc: 0.81716
11 test acc: 0.8337 train loss: 0.007999929081201554 train acc: 0.82534
best:  0.8337
12 test acc: 0.835 train loss: 0.007790929724574089 train acc: 0.8281
best:  0.835
13 test acc: 0.8452 train loss: 0.007541796548664569 train acc: 0.83284
best:  0.8452
14 test acc: 0.8465 train loss: 0.007388235799372196 train acc: 0.83782
best:  0.8465
15 test acc: 0.8471 train loss: 0.007174595769941807 train acc: 0.844
best:  0.8471
16 test acc: 0.8578 train loss: 0.007228288954496384 train acc: 0.8383
best:  0.8578
17 test acc: 0.8492 train loss: 0.007213177942931652 train acc: 0.84222
18 test acc: 0.8533 train loss: 0.007098146364688873 train acc: 0.84278
19 test acc: 0.8456 train loss: 0.007007195523381233 train acc: 0.8467
20 test acc: 0.8536 train loss: 0.00694325388610363 train acc: 0.8481
21 test acc: 0.8576 train loss: 0.0069145806506276134 train acc: 0.84794
22 test acc: 0.8595 train loss: 0.0069984733848273755 train acc: 0.84612
best:  0.8595
23 test acc: 0.8514 train loss: 0.006786134536862373 train acc: 0.85146
24 test acc: 0.8629 train loss: 0.006805362295806408 train acc: 0.85038
best:  0.8629
25 test acc: 0.8574 train loss: 0.006681496705710888 train acc: 0.85388
26 test acc: 0.8649 train loss: 0.006698631097078324 train acc: 0.8525
best:  0.8649
27 test acc: 0.8698 train loss: 0.006599537256062031 train acc: 0.8551
best:  0.8698
28 test acc: 0.8621 train loss: 0.006613610584437847 train acc: 0.85538
29 test acc: 0.8572 train loss: 0.006392722027897835 train acc: 0.85948
30 test acc: 0.8572 train loss: 0.006428868881762028 train acc: 0.86084
31 test acc: 0.8658 train loss: 0.00642351479023695 train acc: 0.85934
32 test acc: 0.8651 train loss: 0.006336283976435661 train acc: 0.86086
33 test acc: 0.8608 train loss: 0.006290472915768623 train acc: 0.86244
34 test acc: 0.8688 train loss: 0.006215209614038468 train acc: 0.86618
35 test acc: 0.8662 train loss: 0.006253548278510571 train acc: 0.86236
36 test acc: 0.8657 train loss: 0.006101650311946869 train acc: 0.86596
37 test acc: 0.8695 train loss: 0.006062372180819511 train acc: 0.86722
38 test acc: 0.8787 train loss: 0.00607726678699255 train acc: 0.86586
best:  0.8787
39 test acc: 0.8668 train loss: 0.005866399959623814 train acc: 0.87176
40 test acc: 0.8753 train loss: 0.005956480467319489 train acc: 0.87076
41 test acc: 0.8656 train loss: 0.005872231630682945 train acc: 0.8705
42 test acc: 0.8785 train loss: 0.005786609843075275 train acc: 0.87328
43 test acc: 0.8767 train loss: 0.005648499478399753 train acc: 0.87546
44 test acc: 0.8674 train loss: 0.005674612700641155 train acc: 0.8761
45 test acc: 0.877 train loss: 0.005638341127634048 train acc: 0.87594
46 test acc: 0.8828 train loss: 0.005542822309732437 train acc: 0.87894
best:  0.8828
47 test acc: 0.8797 train loss: 0.00542400218963623 train acc: 0.88156
48 test acc: 0.8772 train loss: 0.005395003433823585 train acc: 0.8816
49 test acc: 0.8771 train loss: 0.005363647851347924 train acc: 0.88286
50 test acc: 0.8894 train loss: 0.00531391406416893 train acc: 0.8837
best:  0.8894
51 test acc: 0.8865 train loss: 0.0052310515893995765 train acc: 0.88494
52 test acc: 0.8873 train loss: 0.005203392336219549 train acc: 0.88762
53 test acc: 0.8862 train loss: 0.005165792099684477 train acc: 0.887
54 test acc: 0.8866 train loss: 0.005147427155226469 train acc: 0.88794
55 test acc: 0.894 train loss: 0.005077924085259438 train acc: 0.8886
best:  0.894
56 test acc: 0.8885 train loss: 0.004980342307388782 train acc: 0.89146
57 test acc: 0.8853 train loss: 0.0050493172194063666 train acc: 0.8884
58 test acc: 0.8727 train loss: 0.0049061158092319965 train acc: 0.8929
59 test acc: 0.8877 train loss: 0.004830531395226717 train acc: 0.89414
60 test acc: 0.8869 train loss: 0.0047826079258322714 train acc: 0.89524
61 test acc: 0.8861 train loss: 0.00475458443030715 train acc: 0.89626
62 test acc: 0.8884 train loss: 0.004671895124912262 train acc: 0.89882
63 test acc: 0.892 train loss: 0.00462872857823968 train acc: 0.89948
64 test acc: 0.8923 train loss: 0.004615665400773287 train acc: 0.89948
65 test acc: 0.8932 train loss: 0.0045461759942770005 train acc: 0.90178
66 test acc: 0.8961 train loss: 0.0045479636800289155 train acc: 0.90102
best:  0.8961
67 test acc: 0.8932 train loss: 0.00445751213118434 train acc: 0.90192
68 test acc: 0.892 train loss: 0.00442906887203455 train acc: 0.9027
69 test acc: 0.8913 train loss: 0.004358761715143919 train acc: 0.90442
70 test acc: 0.8916 train loss: 0.0043690548019111155 train acc: 0.90394
71 test acc: 0.8979 train loss: 0.004257899616509676 train acc: 0.90626
best:  0.8979
72 test acc: 0.9014 train loss: 0.004231173633635044 train acc: 0.90842
best:  0.9014
73 test acc: 0.9029 train loss: 0.004181195719391108 train acc: 0.90924
best:  0.9029
74 test acc: 0.9008 train loss: 0.0041706917841732504 train acc: 0.90728
75 test acc: 0.8937 train loss: 0.0041279948752373456 train acc: 0.90844
76 test acc: 0.8969 train loss: 0.0040914983817934986 train acc: 0.91084
77 test acc: 0.901 train loss: 0.004028721910566091 train acc: 0.91242
78 test acc: 0.8956 train loss: 0.003942690720409155 train acc: 0.9135
79 test acc: 0.8938 train loss: 0.0039016850566864015 train acc: 0.91566
80 test acc: 0.8975 train loss: 0.003913533248826861 train acc: 0.91398
81 test acc: 0.8955 train loss: 0.0038823787136375906 train acc: 0.91472
82 test acc: 0.9015 train loss: 0.003893975524753332 train acc: 0.91418
83 test acc: 0.9048 train loss: 0.0038446278084069492 train acc: 0.9151
best:  0.9048
84 test acc: 0.9003 train loss: 0.003741102039963007 train acc: 0.91872
85 test acc: 0.891 train loss: 0.0037690601843595505 train acc: 0.91622
86 test acc: 0.9053 train loss: 0.003721363426297903 train acc: 0.91856
best:  0.9053
87 test acc: 0.9045 train loss: 0.0037225173669308423 train acc: 0.9205
88 test acc: 0.896 train loss: 0.003738198018372059 train acc: 0.9186
89 test acc: 0.9028 train loss: 0.0036407196465134623 train acc: 0.92028
90 test acc: 0.8984 train loss: 0.003614774213731289 train acc: 0.9216
91 test acc: 0.9022 train loss: 0.003627876359075308 train acc: 0.92046
92 test acc: 0.9007 train loss: 0.003560045080855489 train acc: 0.92282
93 test acc: 0.9052 train loss: 0.0035285441932082174 train acc: 0.92338
94 test acc: 0.9003 train loss: 0.003525135479271412 train acc: 0.92298
95 test acc: 0.9043 train loss: 0.003557851374447346 train acc: 0.9223
96 test acc: 0.9045 train loss: 0.0035148719342052936 train acc: 0.92284
97 test acc: 0.9037 train loss: 0.0035134295219928025 train acc: 0.92296
98 test acc: 0.9082 train loss: 0.0034960289384424687 train acc: 0.92448
best:  0.9082
99 test acc: 0.9013 train loss: 0.0034137360689043997 train acc: 0.92636
100 test acc: 0.8994 train loss: 0.00351072041451931 train acc: 0.92206
