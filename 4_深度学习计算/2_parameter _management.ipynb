{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数管理: 我们可以通过一些特殊的操作对每层当中的网络当中的参数进行操作\n",
    "# 包括初始化, 访问参数: 用于调试, 诊断和可视化. 以及生成共享层\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6978],\n",
       "        [-0.5541]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们先来看一些具有单隐藏层的多层感知机.\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# create Sequential: a sigual MLP (multilayer perceptron )\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "X = torch.rand(size=(2, 4))\n",
    "output = net(X)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=4, out_features=8, bias=True)\n",
      "OrderedDict([('weight', tensor([[-0.1413,  0.0627, -0.1605, -0.2589, -0.0890, -0.0094, -0.2776, -0.3399]])), ('bias', tensor([-0.1872]))])\n"
     ]
    }
   ],
   "source": [
    "# 参数访问:\n",
    "# 我们可以将Sequential看作一个列表容器, 我们可以通过索引看到其中的各个层的值\n",
    "print(net[0]) # 取出第一层\n",
    "# Linear(in_features=4, out_features=8, bias=True)\n",
    "\n",
    "# 通过state_dict()返回这个层的参数. \n",
    "print(net[2].state_dict())\n",
    "# 返回OrderDict这个数据类型.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([-0.1872], requires_grad=True)\n",
      "tensor([-0.1872])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 目标参数, \n",
    "# 注意, 其实每个参数都表示为参数类的一个实例, 在torch中就是Parameter类\n",
    "print(type(net[2].bias))\n",
    "print(net[2].bias) # 打印bias的属性, 其中包括data, grad两个方面\n",
    "dir(net[2].bias) # 查看bias的attribute\n",
    "# 通过data取出数据: \n",
    "print(net[2].bias.data) # 就是单独的data\n",
    "# 通过grad取出梯度\n",
    "print(net[2].bias.grad) # None, 因为我们还没backward所以还没有计算梯度. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 访问梯度\n",
    "net[2].weight.grad == None # True\n",
    "# 因为我们还没有进行backward所以还没有计算梯度, 所以grad为None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.named_parameters at 0x7fdacfbe8740>\n",
      "('weight', Parameter containing:\n",
      "tensor([[-0.3790,  0.3903,  0.3177, -0.3635],\n",
      "        [ 0.2969,  0.1255,  0.1643,  0.3068],\n",
      "        [-0.1398,  0.0216,  0.4197, -0.3163],\n",
      "        [-0.2209,  0.1267,  0.1261, -0.3238],\n",
      "        [ 0.1069, -0.3582,  0.1677,  0.0526],\n",
      "        [ 0.4365, -0.2825, -0.1169,  0.1730],\n",
      "        [ 0.0491, -0.2142, -0.2083,  0.0711],\n",
      "        [-0.4516,  0.3154,  0.4188,  0.3988]], requires_grad=True)) ('bias', Parameter containing:\n",
      "tensor([-0.2954, -0.4296,  0.4109,  0.0995, -0.4806, -0.4591,  0.2232,  0.2851],\n",
      "       requires_grad=True))\n",
      "==================================================\n",
      "('0.weight', Parameter containing:\n",
      "tensor([[-0.3790,  0.3903,  0.3177, -0.3635],\n",
      "        [ 0.2969,  0.1255,  0.1643,  0.3068],\n",
      "        [-0.1398,  0.0216,  0.4197, -0.3163],\n",
      "        [-0.2209,  0.1267,  0.1261, -0.3238],\n",
      "        [ 0.1069, -0.3582,  0.1677,  0.0526],\n",
      "        [ 0.4365, -0.2825, -0.1169,  0.1730],\n",
      "        [ 0.0491, -0.2142, -0.2083,  0.0711],\n",
      "        [-0.4516,  0.3154,  0.4188,  0.3988]], requires_grad=True)) ('0.bias', Parameter containing:\n",
      "tensor([-0.2954, -0.4296,  0.4109,  0.0995, -0.4806, -0.4591,  0.2232,  0.2851],\n",
      "       requires_grad=True)) ('2.weight', Parameter containing:\n",
      "tensor([[-0.1413,  0.0627, -0.1605, -0.2589, -0.0890, -0.0094, -0.2776, -0.3399]],\n",
      "       requires_grad=True)) ('2.bias', Parameter containing:\n",
      "tensor([-0.1872], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "# 一次性访问所有参数:\n",
    "print(net.named_parameters()) # <generator object Module.named_parameters at 0x7fabcee9d7e0>\n",
    "# 是一个生成器对象, 通过yield返回数值. \n",
    "\n",
    "\n",
    "# 访问net[0]的所有参数:\n",
    "print(*[(name, parm) for name, parm in net[0].named_parameters()])\n",
    "\n",
    "print(\"=\"*50)\n",
    "# 访问net中的所有层, 所有参数:\n",
    "print(*[(name, parm) for name, parm in net.named_parameters()]) # 返回net每层中参数 \n",
    "# *进行解包. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1872])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 因为state_dict()返回的本质是一个字典形式: 所以我们还可以通过key的形式进行访问参数\n",
    "net.state_dict()['2.bias'].data # 通过key来查找"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2720],\n",
       "        [0.2720]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 现在让我们看看通过add_module添加网络层, 这样我们可以指定每层的名称\n",
    "\n",
    "# 我们先定义black1:\n",
    "\n",
    "\n",
    "def black1() -> nn.Sequential:\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(4, 8), nn.ReLU(),\n",
    "        nn.Linear(8, 4), nn.ReLU()\n",
    "    )\n",
    "\n",
    "# 然后定义black2: \n",
    "def black2() -> nn.Module:\n",
    "    net = nn.Sequential() # definite a container \n",
    "    \n",
    "    for i in range(4):\n",
    "        # 嵌套\n",
    "        net.add_module(f\"black: {i}\", black1())\n",
    "        # 和我们直接在Sequential中定义的唯一区别, 就是这里我们可以执行模型的名字\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(black2(), nn.Linear(4, 1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (black: 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (black: 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (black: 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (black: 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 打印网络结构:\n",
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4241, -0.1672, -0.1137,  0.2406, -0.0031,  0.2820, -0.1168, -0.2872])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 由于是层是分层嵌套的, 所以我们也可以通过嵌套列表索引一样访问他们, \n",
    "rgnet[0][1][0].bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0008, -0.0061, -0.0027, -0.0110],\n",
       "         [-0.0191, -0.0167, -0.0239, -0.0059],\n",
       "         [-0.0044, -0.0226,  0.0338, -0.0005],\n",
       "         [-0.0152, -0.0104, -0.0025, -0.0038],\n",
       "         [-0.0014, -0.0142, -0.0046,  0.0094],\n",
       "         [ 0.0047, -0.0020, -0.0060, -0.0052],\n",
       "         [-0.0058, -0.0047,  0.0162,  0.0053],\n",
       "         [ 0.0001, -0.0200, -0.0034, -0.0198]]),\n",
       " tensor(0.))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 我们使用的初始化函数, 都在torch.nn.init.*中, \n",
    "# 一般是: nn.init.***\n",
    "# 参数初始化: 我们如何修改默认的函数, 来获得我们新的参数初始化\n",
    "def init_norm(m: nn.Module):\n",
    "    if type(m) == nn.Linear: # 如果是全连接层\n",
    "        nn.init.normal_(m.weight,  mean=0, std=0.01) # 对w进行正态分布初始化\n",
    "        nn.init.zeros_(m.bias) # 将bias设置为0 \n",
    "        # _表示原地操作. 直接在原来的内存上进行操作\n",
    "\n",
    "# nn.Module提供了一种方式就是可以递归遍历Module中的所有网络层. \n",
    "net.apply(init_norm) # 传入函数句柄, 然后apply即可. \n",
    "\n",
    "net[0].weight.data, net[0].bias.data[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 当然我们还可以将所有参数初始化为给定的参数: 例如初始化为1:\n",
    "def init_constant(m : nn.Module):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "# 在api我们可以将模型参数初始化为一个常量, 但是我们在实际的算法中是不被允许的.\n",
    "# 为什么? 因为神经网络本质上是不断搜索参数的过程, 那么我们需要参数随机化, 然后不同的向着负梯度方向更新\n",
    "# 但是如果将weight更新为同一个常量的话, 那么就导致我们的神经网络不起作用. 并且梯度下降算法难以起到作用\n",
    "# 导致我们的效果变差, 并且weight难以被更新\n",
    "\n",
    "net.apply(init_constant)\n",
    "\n",
    "net[0].weight.data, net[0].bias.data\n",
    "# 可见我们的模型参数就都被更新到了. \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2164,  0.1978, -0.0631, -0.4943],\n",
      "        [ 0.6784,  0.6817,  0.5390,  0.0096],\n",
      "        [-0.1690, -0.4595, -0.6092,  0.4800],\n",
      "        [ 0.1883, -0.3058, -0.4731,  0.4180],\n",
      "        [ 0.6634, -0.4068,  0.5876, -0.5340],\n",
      "        [ 0.4437,  0.1796,  0.1938,  0.0240],\n",
      "        [-0.6116, -0.0047,  0.4692, -0.5452],\n",
      "        [-0.3827, -0.1719,  0.4193,  0.0935]])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "# 我们还可以对某些块应用不同的初始化方法: \n",
    "# 例如我们使用Xavier方法更新第一层, 然后将第三个层初始化常量. \n",
    "\n",
    "# 首先实现一下我们初始化的函数: \n",
    "def init_xavier(m: nn.Module):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        # xavier有两种形式, 一种是uniform, 一种是norm\n",
    "        nn.init.zeros_(m.bias)\n",
    "def init_42(m: nn.Module):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42) \n",
    "        # 为什么是42呢, 因为这是宇宙的答案\n",
    "        # not panic!\n",
    "# 因为net中的子层也是nn.Module. 所以我们同样可以针对某一层进行.apply\n",
    "net[0].apply(init_xavier)\n",
    "net[2].apply(init_42)\n",
    "\n",
    "print(net[0].weight.data)\n",
    "print(net[2].weight.data) # 可见我们的数据都已经被格式化成为42了. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [47], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m         m\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdata \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m m\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mabs() \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[1;32m      9\u001b[0m         \u001b[39m# 保留绝对值 >= 5的那些权重\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m net\u001b[39m.\u001b[39mapply(my_init)\n\u001b[1;32m     12\u001b[0m net[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mweight\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:668\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[39mas well as self. Typical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[39m(see also :ref:`nn-init-doc`).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[39m    )\u001b[39;00m\n\u001b[1;32m    666\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 668\u001b[0m     module\u001b[39m.\u001b[39;49mapply(fn)\n\u001b[1;32m    669\u001b[0m fn(\u001b[39mself\u001b[39m)\n\u001b[1;32m    670\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:669\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m    668\u001b[0m     module\u001b[39m.\u001b[39mapply(fn)\n\u001b[0;32m--> 669\u001b[0m fn(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    670\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "Cell \u001b[0;32mIn [47], line 6\u001b[0m, in \u001b[0;36mmy_init\u001b[0;34m(m)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmy_init\u001b[39m(m):\n\u001b[1;32m      5\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(m) \u001b[39m==\u001b[39m nn\u001b[39m.\u001b[39mLinear:\n\u001b[0;32m----> 6\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39minit:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m[(name, param\u001b[39m.\u001b[39mshape) \u001b[39mfor\u001b[39;00m name, param \u001b[39min\u001b[39;00m m\u001b[39m.\u001b[39;49mnamed_parameters()[\u001b[39m0\u001b[39;49m]])\n\u001b[1;32m      7\u001b[0m         nn\u001b[39m.\u001b[39minit\u001b[39m.\u001b[39muniform_(m\u001b[39m.\u001b[39mweight, \u001b[39m-\u001b[39m\u001b[39m10\u001b[39m, \u001b[39m10\u001b[39m)\n\u001b[1;32m      8\u001b[0m         m\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdata \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m m\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mabs() \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# 自定义初始化, 我们可以使用我们自己的初始化函数, 而不必使用nn.init中的初始化方法. \n",
    "# 我们可以定义我们自己的初始化函数\n",
    "\n",
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\"init:\", *[(name, param.shape) for name, param in m.named_parameters()[0]])\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5\n",
    "        # 保留绝对值 >= 5的那些权重\n",
    "\n",
    "net.apply(my_init)\n",
    "net[2].weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
