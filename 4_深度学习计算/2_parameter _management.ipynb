{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数管理: 我们可以通过一些特殊的操作对每层当中的网络当中的参数进行操作\n",
    "# 包括初始化, 访问参数: 用于调试, 诊断和可视化. 以及生成共享层\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0510],\n",
       "        [0.0482]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们先来看一些具有单隐藏层的多层感知机.\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# create Sequential: a sigual MLP (multilayer perceptron )\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "X = torch.rand(size=(2, 4))\n",
    "output = net(X)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=4, out_features=8, bias=True)\n",
      "OrderedDict([('weight', tensor([[-0.1367,  0.3375,  0.0475,  0.1561,  0.2346, -0.0513, -0.1979,  0.2334]])), ('bias', tensor([0.0429]))])\n"
     ]
    }
   ],
   "source": [
    "# 参数访问:\n",
    "# 我们可以将Sequential看作一个列表容器, 我们可以通过索引看到其中的各个层的值\n",
    "print(net[0]) # 取出第一层\n",
    "# Linear(in_features=4, out_features=8, bias=True)\n",
    "\n",
    "# 通过state_dict()返回这个层的参数. \n",
    "print(net[2].state_dict())\n",
    "# 返回OrderDict这个数据类型.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([0.0429], requires_grad=True)\n",
      "tensor([0.0429])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 目标参数, \n",
    "# 注意, 其实每个参数都表示为参数类的一个实例, 在torch中就是Parameter类\n",
    "print(type(net[2].bias))\n",
    "print(net[2].bias) # 打印bias的属性, 其中包括data, grad两个方面\n",
    "dir(net[2].bias) # 查看bias的attribute\n",
    "# 通过data取出数据: \n",
    "print(net[2].bias.data) # 就是单独的data\n",
    "# 通过grad取出梯度\n",
    "print(net[2].bias.grad) # None, 因为我们还没backward所以还没有计算梯度. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 访问梯度\n",
    "net[2].weight.grad == None # True\n",
    "# 因为我们还没有进行backward所以还没有计算梯度, 所以grad为None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.named_parameters at 0x7f2253bf4660>\n",
      "('weight', Parameter containing:\n",
      "tensor([[-0.3137,  0.4447,  0.1632, -0.0665],\n",
      "        [ 0.1398, -0.3636, -0.1345,  0.1400],\n",
      "        [-0.3024,  0.4709,  0.4361, -0.0838],\n",
      "        [ 0.2103, -0.4333,  0.3128, -0.2857],\n",
      "        [-0.2681,  0.3009,  0.0493, -0.4782],\n",
      "        [-0.3141,  0.0650,  0.3362,  0.2607],\n",
      "        [-0.2926,  0.2631, -0.1591, -0.3237],\n",
      "        [ 0.4916, -0.3872, -0.1101, -0.4044]], requires_grad=True)) ('bias', Parameter containing:\n",
      "tensor([-0.3313, -0.1666,  0.0280, -0.0343, -0.0414,  0.0065, -0.4216, -0.1318],\n",
      "       requires_grad=True))\n",
      "==================================================\n",
      "('0.weight', Parameter containing:\n",
      "tensor([[-0.3137,  0.4447,  0.1632, -0.0665],\n",
      "        [ 0.1398, -0.3636, -0.1345,  0.1400],\n",
      "        [-0.3024,  0.4709,  0.4361, -0.0838],\n",
      "        [ 0.2103, -0.4333,  0.3128, -0.2857],\n",
      "        [-0.2681,  0.3009,  0.0493, -0.4782],\n",
      "        [-0.3141,  0.0650,  0.3362,  0.2607],\n",
      "        [-0.2926,  0.2631, -0.1591, -0.3237],\n",
      "        [ 0.4916, -0.3872, -0.1101, -0.4044]], requires_grad=True)) ('0.bias', Parameter containing:\n",
      "tensor([-0.3313, -0.1666,  0.0280, -0.0343, -0.0414,  0.0065, -0.4216, -0.1318],\n",
      "       requires_grad=True)) ('2.weight', Parameter containing:\n",
      "tensor([[-0.1367,  0.3375,  0.0475,  0.1561,  0.2346, -0.0513, -0.1979,  0.2334]],\n",
      "       requires_grad=True)) ('2.bias', Parameter containing:\n",
      "tensor([0.0429], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "# 一次性访问所有参数:\n",
    "print(net.named_parameters()) # <generator object Module.named_parameters at 0x7fabcee9d7e0>\n",
    "# 是一个生成器对象, 通过yield返回数值. \n",
    "\n",
    "\n",
    "# 访问net[0]的所有参数:\n",
    "print(*[(name, parm) for name, parm in net[0].named_parameters()])\n",
    "\n",
    "print(\"=\"*50)\n",
    "# 访问net中的所有层, 所有参数:\n",
    "print(*[(name, parm) for name, parm in net.named_parameters()]) # 返回net每层中参数 \n",
    "# *进行解包. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0429])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 因为state_dict()返回的本质是一个字典形式: 所以我们还可以通过key的形式进行访问参数\n",
    "net.state_dict()['2.bias'].data # 通过key来查找"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3413],\n",
       "        [-0.3413]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 现在让我们看看通过add_module添加网络层, 这样我们可以指定每层的名称\n",
    "\n",
    "# 我们先定义black1:\n",
    "\n",
    "\n",
    "def black1() -> nn.Sequential:\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(4, 8), nn.ReLU(),\n",
    "        nn.Linear(8, 4), nn.ReLU()\n",
    "    )\n",
    "\n",
    "# 然后定义black2: \n",
    "def black2() -> nn.Module:\n",
    "    net = nn.Sequential() # definite a container \n",
    "    \n",
    "    for i in range(4):\n",
    "        # 嵌套\n",
    "        net.add_module(f\"black: {i}\", black1())\n",
    "        # 和我们直接在Sequential中定义的唯一区别, 就是这里我们可以执行模型的名字\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(black2(), nn.Linear(4, 1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (black: 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (black: 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (black: 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (black: 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 打印网络结构:\n",
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3868,  0.0399,  0.1805,  0.2331,  0.1051,  0.1194,  0.1589,  0.3595])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 由于是层是分层嵌套的, 所以我们也可以通过嵌套列表索引一样访问他们, \n",
    "rgnet[0][1][0].bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数初始化: 我们如何修改默认的函数, 来获得我们新的参数初始化\n",
    "def init_norm(m: nn.Module):\n",
    "    if type(m) == nn.Linear: # 如果是全连接层\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01) # 对w进行正态分布初始化"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
