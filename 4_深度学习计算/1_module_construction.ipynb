{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F # 里面定义了一些无参的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 20]), torch.Size([2, 10]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sequential是一种特殊的Module. 他也继承nn.Module. 一会我们会看到它是如何被实现的.\n",
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10)) # 创建一多层感知机模型\n",
    "# Sequential是一个容器. 主要\n",
    "# 我们通过sequential定义了一个块, 里面的每一个Module是一个层. \n",
    "X = torch.rand(2, 20) # 随机生成模拟数据\n",
    "out = net(X)\n",
    "X.shape, out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义块:\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256) # hidden layer\n",
    "        self.out = nn.Linear(256, 10) # output layer \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.out(F.relu(self.hidden(x)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 10]), torch.Size([2, 20]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用我们创建的块, 创建网络模型\n",
    "net = MLP() # 实例化\n",
    "\n",
    "out = net(X) # 实例调用__call_方法, 直接调用forward\n",
    "\n",
    "out.shape, X.shape # x经过net得到了输出."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 顺序块: 现在我们可以更加仔细地看看Sequential类是如何工作的.\n",
    "# 我们利用Module来实现一下: \n",
    "# from torch import Module\n",
    "\n",
    "\n",
    "# 自定义的Sequential:\n",
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args,) -> None:\n",
    "        super().__init__(*args,)\n",
    "        for idx, module in enumerate(args): # args is a tuple. *args are scattered data item. \n",
    "            self._modules[str(idx)] = module\n",
    "            # _modules是torch.nn.Module中的一个特殊变量, 是一个orderedDict()即有序字典.\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for module in self._modules.values():\n",
    "            x = module(x)\n",
    "        return x\n",
    "\n",
    " \n",
    "# class MySequential(nn.Module):\n",
    "#     def __init__(self, *args) -> None:\n",
    "#         super().__init__()\n",
    "#         for idx, module in enumerate(args):\n",
    "#             self._modules[str(idx)] = module # __modules是torch中的一个特殊变量\n",
    "#             # 是一个迭代器, orderedDict()\n",
    "#         # for block in args:\n",
    "#             # self._modules[block] = block # 这是一种特殊的\n",
    "\n",
    "#     def forward(self, X):\n",
    "#          # OrderDict保证了按照成员添加的顺序遍历他们.\n",
    "#         for block in self._modules.values(): # 取出\n",
    "#             X = block(X)\n",
    "        \n",
    "#         return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 10]), torch.Size([2, 20]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 几乎和我们使用Sequential是一样的, 我们传入的也是层参数, 然后返回一个实例\n",
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "\n",
    "out = net(X)\n",
    "\n",
    "out.shape, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有了Sequential我们为什么还要使用Module. 这种显示创建的形式.\n",
    "# 因为我们可以在init和forward中做一些代码操作. 这样使我们的网络模型更加灵活\n",
    "class FixedHiddenMLP(nn.Module): # 固定隐藏层\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # 不计算其中的权重梯度. 那么更新时在训练期间保持不变.\n",
    "        self.rand_weight = torch.rand(size=(20, 20), requires_grad=False) # 不计算梯度, 也就是固定住了.\n",
    "        # 因为不更新, 所以是随机加上了一层参数.\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        # 使用functional中的relu函数:\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1) # mm: matmul就是矩阵乘积\n",
    "        # 复用Linear层, 这表示两个层之间共享参数. \n",
    "        # 当我们想要将两个层的参数共享的时候, 我们就是用这种方式w\n",
    "        X = self.linear(X)\n",
    "\n",
    "        # 控制流:\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0775, grad_fn=<SumBackward0>), torch.Size([]), torch.Size([2, 20]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试, 在这个FixedHiddenMLP Module中, 权重: rand_weight永远不会被更新, 从初始化后就一直维持常量的状态.\n",
    "net = FixedHiddenMLP() # 实例\n",
    "out = net(X)\n",
    "\n",
    "out, out.shape, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): NestMLP(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=20, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (linear): Linear(in_features=32, out_features=16, bias=True)\n",
      "  )\n",
      "  (1): Linear(in_features=16, out_features=20, bias=True)\n",
      "  (2): FixedHiddenMLP(\n",
      "    (linear): Linear(in_features=20, out_features=20, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.0981, grad_fn=<SumBackward0>), torch.Size([]), torch.Size([2, 20]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们可以混合块和Sequential去使用.\n",
    "# 因为实际上无论是我们自定义的块, 还是Sequential都是nn.Module的子类. 所以我们当然可以嵌套使用\n",
    "class NestMLP(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(20, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 32), nn.ReLU()\n",
    "        )\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n",
    "# 可以看见, 我们首先经过NestMLP()的forward, 然后进入linear, 最后进入我们上面定义的FixdHiddenMLP层\n",
    "\n",
    "out = chimera(X)\n",
    "print(chimera) # 打印网络结构. \n",
    "out, out.shape, X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 一个块可以由许多层组成；一个块可以由许多块组成。\n",
    "* 块可以包含代码。\n",
    "* 块负责大量的内部处理，包括参数初始化和反向传播。\n",
    "* 层和块的顺序连接由`Sequential`块处理。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 如果将`MySequential`中存储块的方式更改为Python列表，会出现什么样的问题？\n",
    "1. 实现一个块，它以两个块为参数，例如`net1`和`net2`，并返回前向传播中两个网络的串联输出。这也被称为平行块。\n",
    "1. 假设你想要连接同一网络的多个实例。实现一个函数，该函数生成同一个块的多个实例，并在此基础上构建更大的网络。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2:\n",
    "class ParallelNet(nn.Module):\n",
    "    def __init__(self, net1, net2) -> None:\n",
    "        super().__init__()\n",
    "        self.net1 = net1\n",
    "        self.net2 = net2\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.net2(self.net1(X))\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = ParallelNet(\n",
    "    nn.Linear(20, 20),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "net(torch.rand(size=(2, 20))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
